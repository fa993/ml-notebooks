{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71e5c430-2c89-4e03-acc9-4f6cbb003fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.2.0.dev20231001 CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01742c9d-97be-4224-96e8-8a4307da3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "801a4e11-61f9-43ae-be12-ec5fb12c8996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('/Users/ameyakore/Downloads/last-6.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58690e-0cc0-427f-bcf5-2a3869e3b22f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train(data=\"/Users/ameyakore/workspaces/ai-workspace/dvl/notebooks/data/car_bus_truck_Detection/data.yaml\", epochs=14, cache=\"ram\", batch=64) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b580d-1458-4be2-90f0-a74be407b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1p = pathlib.Path(\"data/car_bus_truck_Detection_2/test/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3efda-a778-4b2a-b60f-b387c7fd58a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for it in img1p.iterdir():\n",
    "    out = model.predict(it, save_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7333095-b06a-41b8-9527-46dba31238c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(source=\"data/car_bus_truck_Detection_2/test/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb99758c-120c-4be8-a569-4dd6e8eb5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: 'car', 1: 'large_bus', 2: 'medium_truck'}\n",
    "# classes = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "COLORS = np.array([[255.0, 0, 0], [0, 255, 0], [0, 0, 255]])\n",
    "# COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x2, y2):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x2,y2), color, 2)\n",
    "\n",
    "    cv2.putText(img, label + \": \"+ \"{:.2f}\".format(confidence), (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def draw_bounding_image(img, mask, class_id, confidence, x, y, x2, y2):\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    mask[y:y2,x:x2] = img[y:y2,x:x2]\n",
    "\n",
    "    cv2.rectangle(mask, (x,y), (x2,y2), color, 2)\n",
    "\n",
    "    cv2.putText(mask, label + \": \"+ \"{:.2f}\".format(confidence), (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2674606-fd97-49d5-aff8-4a495428fbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 12:37:28.530 Python[67928:12013149] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "\n",
      "image 1/1 /Users/ameyakore/Downloads/images-2.jpeg: 640x640 1 large_bus, 75.8ms\n",
      "Speed: 1.2ms preprocess, 75.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3212])\n",
      "data: tensor([[1.3838e+02, 1.7371e+02, 2.7046e+02, 3.6787e+02, 3.2118e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[204.4205, 270.7924, 132.0867, 194.1578]])\n",
      "xywhn: tensor([[0.3194, 0.4231, 0.2064, 0.3034]])\n",
      "xyxy: tensor([[138.3771, 173.7135, 270.4639, 367.8713]])\n",
      "xyxyn: tensor([[0.2162, 0.2714, 0.4226, 0.5748]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstimg = pathlib.Path(\"/Users/ameyakore/Downloads/images-2.jpeg\")\n",
    "out = model.predict(tstimg, show=True)\n",
    "print(out[0].boxes)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987dab13-1cdb-46d9-979c-a88866f28538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_draw_on_frame(frame, imgsz=(640, 640)):\n",
    "    out1 = model(frame, conf=0.01, classes=[0,2], imgsz=imgsz)\n",
    "    out2 = model(frame, conf=0.75, classes=[1], imgsz=imgsz)\n",
    "    # out3 = model(frame, conf=0.1, classes=[2])\n",
    "    for ou in out1:\n",
    "        for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "                draw_bounding_box(frame, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "    \n",
    "    for ou in out2:\n",
    "        for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "                draw_bounding_box(frame, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "    \n",
    "    \n",
    "    # for ou in out3:\n",
    "    #     for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "    #             draw_bounding_box(frame, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "\n",
    "    return frame\n",
    "\n",
    "def predict_normal(frame):\n",
    "    out1 = model(frame)\n",
    "    for ou in out1:\n",
    "        print(ou.boxes)\n",
    "        for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "                draw_bounding_box(frame, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "def predict_and_draw_on_mask(frame, mask):\n",
    "    out1 = model(frame, conf=0.01, classes=[0,2])\n",
    "    out2 = model(frame, conf=0.75, classes=[1])\n",
    "    # out3 = model(frame, conf=0.1, classes=[2])\n",
    "    for ou in out1:\n",
    "        for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "                draw_bounding_image(frame, mask, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "    \n",
    "    for ou in out2:\n",
    "        for c, co, xywh in zip(ou.boxes.cls, ou.boxes.conf, ou.boxes.xyxy):\n",
    "                draw_bounding_image(frame, mask, int(c.item()), co.item(), int(xywh[0].item()), int(xywh[1].item()),int(xywh[2].item()), int(xywh[3].item()))\n",
    "    return mask\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f0eed5-ec03-468b-94e3-d69f539d9aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 12 cars, 5 medium_trucks, 77.1ms\n",
      "Speed: 2.2ms preprocess, 77.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 67.0ms\n",
      "Speed: 1.1ms preprocess, 67.0ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = cv2.imread(\"/Users/ameyakore/Downloads/images-2.jpeg\")\n",
    "\n",
    "frame = predict_and_draw_on_frame(frame)\n",
    "\n",
    "cv2.imwrite(\"tst.jpeg\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab747f39-7e5b-4ab9-a4c5-114dd39564da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 19 cars, 6 medium_trucks, 72.1ms\n",
      "Speed: 2.0ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 68.2ms\n",
      "Speed: 1.3ms preprocess, 68.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = cv2.imread(\"/Users/ameyakore/Downloads/originaltest.jpg\")\n",
    "\n",
    "frame = predict_and_draw_on_frame(frame)\n",
    "\n",
    "cv2.imwrite(\"tst.jpeg\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140cb116-490e-4c18-b127-970f822dc319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 72.0ms\n",
      "Speed: 1.7ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 68.0ms\n",
      "Speed: 1.3ms preprocess, 68.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8990, 0.8861, 0.8646, 0.8281])\n",
      "data: tensor([[173.4313, 159.5974, 363.0793, 612.4052,   0.8990,   1.0000],\n",
      "        [253.3825,   0.0000, 429.2690, 204.0071,   0.8861,   1.0000],\n",
      "        [430.4995,   0.0000, 551.3918, 189.9637,   0.8646,   1.0000],\n",
      "        [552.3604,   0.0000, 639.7989, 154.3849,   0.8281,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[268.2553, 386.0013, 189.6479, 452.8078],\n",
      "        [341.3257, 102.0036, 175.8865, 204.0071],\n",
      "        [490.9456,  94.9818, 120.8924, 189.9637],\n",
      "        [596.0797,  77.1925,  87.4385, 154.3849]])\n",
      "xywhn: tensor([[0.4191, 0.6031, 0.2963, 0.7075],\n",
      "        [0.5333, 0.1594, 0.2748, 0.3188],\n",
      "        [0.7671, 0.1484, 0.1889, 0.2968],\n",
      "        [0.9314, 0.1206, 0.1366, 0.2412]])\n",
      "xyxy: tensor([[173.4313, 159.5974, 363.0793, 612.4052],\n",
      "        [253.3825,   0.0000, 429.2690, 204.0071],\n",
      "        [430.4995,   0.0000, 551.3918, 189.9637],\n",
      "        [552.3604,   0.0000, 639.7989, 154.3849]])\n",
      "xyxyn: tensor([[0.2710, 0.2494, 0.5673, 0.9569],\n",
      "        [0.3959, 0.0000, 0.6707, 0.3188],\n",
      "        [0.6727, 0.0000, 0.8615, 0.2968],\n",
      "        [0.8631, 0.0000, 0.9997, 0.2412]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8996])\n",
      "data: tensor([[113.9853, 204.5158, 447.5086, 490.9302,   0.8996,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[280.7469, 347.7230, 333.5233, 286.4144]])\n",
      "xywhn: tensor([[0.4387, 0.5433, 0.5211, 0.4475]])\n",
      "xyxy: tensor([[113.9853, 204.5158, 447.5086, 490.9302]])\n",
      "xyxyn: tensor([[0.1781, 0.3196, 0.6992, 0.7671]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 large_buss, 87.8ms\n",
      "Speed: 3.1ms preprocess, 87.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 65.5ms\n",
      "Speed: 1.1ms preprocess, 65.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.8874, 0.6200, 0.6159])\n",
      "data: tensor([[2.2721e+02, 4.3203e+02, 4.0893e+02, 5.6918e+02, 8.8739e-01, 1.0000e+00],\n",
      "        [1.8278e+01, 1.3863e-02, 5.3664e+01, 1.9548e+01, 6.2000e-01, 1.0000e+00],\n",
      "        [1.3053e-01, 1.7836e-01, 1.6837e+01, 3.6547e+01, 6.1595e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[318.0686, 500.6044, 181.7160, 137.1572],\n",
      "        [ 35.9708,   9.7807,  35.3865,  19.5337],\n",
      "        [  8.4838,  18.3627,  16.7066,  36.3686]])\n",
      "xywhn: tensor([[0.4970, 0.7822, 0.2839, 0.2143],\n",
      "        [0.0562, 0.0153, 0.0553, 0.0305],\n",
      "        [0.0133, 0.0287, 0.0261, 0.0568]])\n",
      "xyxy: tensor([[2.2721e+02, 4.3203e+02, 4.0893e+02, 5.6918e+02],\n",
      "        [1.8278e+01, 1.3863e-02, 5.3664e+01, 1.9548e+01],\n",
      "        [1.3053e-01, 1.7836e-01, 1.6837e+01, 3.6547e+01]])\n",
      "xyxyn: tensor([[3.5502e-01, 6.7504e-01, 6.3895e-01, 8.8935e-01],\n",
      "        [2.8559e-02, 2.1660e-05, 8.3850e-02, 3.0543e-02],\n",
      "        [2.0395e-04, 2.7869e-04, 2.6308e-02, 5.7105e-02]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9083, 0.9017])\n",
      "data: tensor([[120.9042, 185.7916, 467.6229, 487.3267,   0.9083,   1.0000],\n",
      "        [  0.0000, 385.1597, 340.4526, 639.1182,   0.9017,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[294.2635, 336.5592, 346.7187, 301.5351],\n",
      "        [170.2263, 512.1389, 340.4526, 253.9585]])\n",
      "xywhn: tensor([[0.4598, 0.5259, 0.5417, 0.4711],\n",
      "        [0.2660, 0.8002, 0.5320, 0.3968]])\n",
      "xyxy: tensor([[120.9042, 185.7916, 467.6229, 487.3267],\n",
      "        [  0.0000, 385.1597, 340.4526, 639.1182]])\n",
      "xyxyn: tensor([[0.1889, 0.2903, 0.7307, 0.7614],\n",
      "        [0.0000, 0.6018, 0.5320, 0.9986]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 67.4ms\n",
      "Speed: 1.6ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 66.5ms\n",
      "Speed: 1.6ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9178, 0.8595])\n",
      "data: tensor([[178.9827, 256.0029, 444.2211, 449.2852,   0.9178,   1.0000],\n",
      "        [578.5108, 306.1109, 640.0000, 486.4069,   0.8595,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[311.6019, 352.6440, 265.2385, 193.2823],\n",
      "        [609.2554, 396.2589,  61.4892, 180.2960]])\n",
      "xywhn: tensor([[0.4869, 0.5510, 0.4144, 0.3020],\n",
      "        [0.9520, 0.6192, 0.0961, 0.2817]])\n",
      "xyxy: tensor([[178.9827, 256.0029, 444.2211, 449.2852],\n",
      "        [578.5108, 306.1109, 640.0000, 486.4069]])\n",
      "xyxyn: tensor([[0.2797, 0.4000, 0.6941, 0.7020],\n",
      "        [0.9039, 0.4783, 1.0000, 0.7600]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8932, 0.2690])\n",
      "data: tensor([[1.2587e+02, 2.5967e+02, 4.7744e+02, 4.8606e+02, 8.9321e-01, 1.0000e+00],\n",
      "        [6.0293e+02, 8.1609e-01, 6.3999e+02, 1.5397e+02, 2.6896e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[301.6542, 372.8646, 351.5758, 226.3971],\n",
      "        [621.4558,  77.3943,  37.0605, 153.1565]])\n",
      "xywhn: tensor([[0.4713, 0.5826, 0.5493, 0.3537],\n",
      "        [0.9710, 0.1209, 0.0579, 0.2393]])\n",
      "xyxy: tensor([[125.8663, 259.6661, 477.4421, 486.0632],\n",
      "        [602.9255,   0.8161, 639.9861, 153.9725]])\n",
      "xyxyn: tensor([[0.1967, 0.4057, 0.7460, 0.7595],\n",
      "        [0.9421, 0.0013, 1.0000, 0.2406]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 5 large_buss, 75.7ms\n",
      "Speed: 1.2ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 72.6ms\n",
      "Speed: 1.0ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9064, 0.8930, 0.8848, 0.8556, 0.8545])\n",
      "data: tensor([[5.3066e+02, 2.3768e+02, 6.2286e+02, 3.7449e+02, 9.0643e-01, 1.0000e+00],\n",
      "        [3.4483e+02, 1.7696e+02, 4.5491e+02, 2.6904e+02, 8.9304e-01, 1.0000e+00],\n",
      "        [2.6816e+02, 4.5594e+02, 4.1866e+02, 5.6568e+02, 8.8482e-01, 1.0000e+00],\n",
      "        [1.4418e-01, 1.6242e+02, 3.5093e+01, 2.1676e+02, 8.5564e-01, 1.0000e+00],\n",
      "        [7.3345e-02, 2.1874e+02, 4.3808e+01, 2.8150e+02, 8.5447e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[576.7637, 306.0844,  92.2026, 136.8045],\n",
      "        [399.8730, 222.9955, 110.0774,  92.0797],\n",
      "        [343.4110, 510.8080, 150.5024, 109.7352],\n",
      "        [ 17.6188, 189.5885,  34.9492,  54.3358],\n",
      "        [ 21.9409, 250.1212,  43.7351,  62.7526]])\n",
      "xywhn: tensor([[0.9012, 0.4783, 0.1441, 0.2138],\n",
      "        [0.6248, 0.3484, 0.1720, 0.1439],\n",
      "        [0.5366, 0.7981, 0.2352, 0.1715],\n",
      "        [0.0275, 0.2962, 0.0546, 0.0849],\n",
      "        [0.0343, 0.3908, 0.0683, 0.0981]])\n",
      "xyxy: tensor([[5.3066e+02, 2.3768e+02, 6.2286e+02, 3.7449e+02],\n",
      "        [3.4483e+02, 1.7696e+02, 4.5491e+02, 2.6904e+02],\n",
      "        [2.6816e+02, 4.5594e+02, 4.1866e+02, 5.6568e+02],\n",
      "        [1.4418e-01, 1.6242e+02, 3.5093e+01, 2.1676e+02],\n",
      "        [7.3345e-02, 2.1874e+02, 4.3808e+01, 2.8150e+02]])\n",
      "xyxyn: tensor([[8.2916e-01, 3.7138e-01, 9.7323e-01, 5.8514e-01],\n",
      "        [5.3880e-01, 2.7649e-01, 7.1080e-01, 4.2037e-01],\n",
      "        [4.1900e-01, 7.1241e-01, 6.5416e-01, 8.8387e-01],\n",
      "        [2.2528e-04, 2.5378e-01, 5.4833e-02, 3.3868e-01],\n",
      "        [1.1460e-04, 3.4179e-01, 6.8451e-02, 4.3984e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9141, 0.5505])\n",
      "data: tensor([[2.6602e+02, 1.5061e+02, 4.4384e+02, 2.9888e+02, 9.1414e-01, 1.0000e+00],\n",
      "        [6.1908e+02, 2.1913e+02, 6.4000e+02, 3.3894e+02, 5.5047e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[354.9318, 224.7430, 177.8145, 148.2641],\n",
      "        [629.5422, 279.0391,  20.9156, 119.8109]])\n",
      "xywhn: tensor([[0.5546, 0.3512, 0.2778, 0.2317],\n",
      "        [0.9837, 0.4360, 0.0327, 0.1872]])\n",
      "xyxy: tensor([[266.0246, 150.6110, 443.8391, 298.8751],\n",
      "        [619.0844, 219.1337, 640.0000, 338.9446]])\n",
      "xyxyn: tensor([[0.4157, 0.2353, 0.6935, 0.4670],\n",
      "        [0.9673, 0.3424, 1.0000, 0.5296]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 71.0ms\n",
      "Speed: 1.5ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 72.6ms\n",
      "Speed: 1.4ms preprocess, 72.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9109])\n",
      "data: tensor([[ 63.3879, 178.9312, 440.7492, 484.3356,   0.9109,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[252.0686, 331.6334, 377.3613, 305.4044]])\n",
      "xywhn: tensor([[0.3939, 0.5182, 0.5896, 0.4772]])\n",
      "xyxy: tensor([[ 63.3879, 178.9312, 440.7492, 484.3356]])\n",
      "xyxyn: tensor([[0.0990, 0.2796, 0.6887, 0.7568]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8935])\n",
      "data: tensor([[ 53.6361, 206.8383, 483.7953, 482.9943,   0.8935,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[268.7157, 344.9163, 430.1592, 276.1560]])\n",
      "xywhn: tensor([[0.4199, 0.5389, 0.6721, 0.4315]])\n",
      "xyxy: tensor([[ 53.6361, 206.8383, 483.7953, 482.9943]])\n",
      "xyxyn: tensor([[0.0838, 0.3232, 0.7559, 0.7547]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 71.8ms\n",
      "Speed: 1.8ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 72.5ms\n",
      "Speed: 1.1ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9061])\n",
      "data: tensor([[112.9746, 259.5027, 418.4991, 453.8718,   0.9061,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[265.7368, 356.6873, 305.5245, 194.3690]])\n",
      "xywhn: tensor([[0.4152, 0.5573, 0.4774, 0.3037]])\n",
      "xyxy: tensor([[112.9746, 259.5027, 418.4991, 453.8718]])\n",
      "xyxyn: tensor([[0.1765, 0.4055, 0.6539, 0.7092]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9159, 0.9025, 0.4271])\n",
      "data: tensor([[3.6451e+02, 1.0402e+02, 4.5664e+02, 2.9220e+02, 9.1594e-01, 1.0000e+00],\n",
      "        [2.4608e+02, 4.0070e+02, 4.3254e+02, 5.8953e+02, 9.0248e-01, 1.0000e+00],\n",
      "        [1.7789e+00, 4.0123e+02, 1.8100e+02, 5.1986e+02, 4.2711e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[410.5750, 198.1125,  92.1230, 188.1788],\n",
      "        [339.3083, 495.1132, 186.4657, 188.8264],\n",
      "        [ 91.3906, 460.5442, 179.2234, 118.6365]])\n",
      "xywhn: tensor([[0.6415, 0.3096, 0.1439, 0.2940],\n",
      "        [0.5302, 0.7736, 0.2914, 0.2950],\n",
      "        [0.1428, 0.7196, 0.2800, 0.1854]])\n",
      "xyxy: tensor([[364.5134, 104.0232, 456.6365, 292.2019],\n",
      "        [246.0755, 400.7000, 432.5412, 589.5264],\n",
      "        [  1.7789, 401.2259, 181.0023, 519.8624]])\n",
      "xyxyn: tensor([[0.5696, 0.1625, 0.7135, 0.4566],\n",
      "        [0.3845, 0.6261, 0.6758, 0.9211],\n",
      "        [0.0028, 0.6269, 0.2828, 0.8123]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 74.1ms\n",
      "Speed: 1.3ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8951])\n",
      "data: tensor([[268.7900, 256.7631, 396.8054, 349.0162,   0.8951,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[332.7977, 302.8896, 128.0154,  92.2532]])\n",
      "xywhn: tensor([[0.5200, 0.4733, 0.2000, 0.1441]])\n",
      "xyxy: tensor([[268.7900, 256.7631, 396.8054, 349.0162]])\n",
      "xyxyn: tensor([[0.4200, 0.4012, 0.6200, 0.5453]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 large_buss, 71.1ms\n",
      "Speed: 1.1ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 69.5ms\n",
      "Speed: 1.5ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9115, 0.7963])\n",
      "data: tensor([[113.9661, 198.6807, 350.0408, 498.5265,   0.9115,   1.0000],\n",
      "        [266.6914, 409.7823, 367.1240, 607.0566,   0.7963,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[232.0034, 348.6036, 236.0746, 299.8458],\n",
      "        [316.9077, 508.4195, 100.4326, 197.2744]])\n",
      "xywhn: tensor([[0.3625, 0.5447, 0.3689, 0.4685],\n",
      "        [0.4952, 0.7944, 0.1569, 0.3082]])\n",
      "xyxy: tensor([[113.9661, 198.6807, 350.0408, 498.5265],\n",
      "        [266.6914, 409.7823, 367.1240, 607.0566]])\n",
      "xyxyn: tensor([[0.1781, 0.3104, 0.5469, 0.7789],\n",
      "        [0.4167, 0.6403, 0.5736, 0.9485]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9050, 0.4925])\n",
      "data: tensor([[1.6454e+02, 2.1592e+02, 4.4878e+02, 4.0143e+02, 9.0501e-01, 1.0000e+00],\n",
      "        [1.1450e-01, 2.6307e+02, 2.1153e+01, 4.2215e+02, 4.9246e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[306.6608, 308.6740, 284.2365, 185.5077],\n",
      "        [ 10.6337, 342.6094,  21.0384, 159.0720]])\n",
      "xywhn: tensor([[0.4792, 0.4823, 0.4441, 0.2899],\n",
      "        [0.0166, 0.5353, 0.0329, 0.2486]])\n",
      "xyxy: tensor([[1.6454e+02, 2.1592e+02, 4.4878e+02, 4.0143e+02],\n",
      "        [1.1450e-01, 2.6307e+02, 2.1153e+01, 4.2215e+02]])\n",
      "xyxyn: tensor([[2.5710e-01, 3.3738e-01, 7.0122e-01, 6.2723e-01],\n",
      "        [1.7890e-04, 4.1105e-01, 3.3051e-02, 6.5960e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 72.6ms\n",
      "Speed: 1.7ms preprocess, 72.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 large_buss, 66.9ms\n",
      "Speed: 1.0ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9005, 0.3025])\n",
      "data: tensor([[1.5192e+02, 1.8196e+02, 5.2146e+02, 5.7975e+02, 9.0051e-01, 1.0000e+00],\n",
      "        [1.3886e-01, 8.4038e-02, 3.9169e+01, 8.3866e+01, 3.0249e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[336.6913, 380.8574, 369.5438, 397.7874],\n",
      "        [ 19.6538,  41.9748,  39.0299,  83.7815]])\n",
      "xywhn: tensor([[0.5261, 0.5951, 0.5774, 0.6215],\n",
      "        [0.0307, 0.0656, 0.0610, 0.1309]])\n",
      "xyxy: tensor([[1.5192e+02, 1.8196e+02, 5.2146e+02, 5.7975e+02],\n",
      "        [1.3886e-01, 8.4038e-02, 3.9169e+01, 8.3866e+01]])\n",
      "xyxyn: tensor([[2.3737e-01, 2.8432e-01, 8.1479e-01, 9.0586e-01],\n",
      "        [2.1697e-04, 1.3131e-04, 6.1201e-02, 1.3104e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8870, 0.8866, 0.8862, 0.7990])\n",
      "data: tensor([[3.1851e+02, 2.0429e+02, 4.5035e+02, 3.1383e+02, 8.8696e-01, 1.0000e+00],\n",
      "        [1.5996e+02, 2.5734e+02, 2.7976e+02, 3.7893e+02, 8.8657e-01, 1.0000e+00],\n",
      "        [2.5590e+02, 3.9283e+02, 3.3651e+02, 5.4716e+02, 8.8617e-01, 1.0000e+00],\n",
      "        [2.1723e-01, 1.6965e+02, 4.0952e+01, 2.4641e+02, 7.9896e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[384.4278, 259.0580, 131.8363, 109.5368],\n",
      "        [219.8637, 318.1323, 119.7995, 121.5924],\n",
      "        [296.2050, 469.9990,  80.6157, 154.3307],\n",
      "        [ 20.5844, 208.0306,  40.7344,  76.7621]])\n",
      "xywhn: tensor([[0.6007, 0.4048, 0.2060, 0.1712],\n",
      "        [0.3435, 0.4971, 0.1872, 0.1900],\n",
      "        [0.4628, 0.7344, 0.1260, 0.2411],\n",
      "        [0.0322, 0.3250, 0.0636, 0.1199]])\n",
      "xyxy: tensor([[3.1851e+02, 2.0429e+02, 4.5035e+02, 3.1383e+02],\n",
      "        [1.5996e+02, 2.5734e+02, 2.7976e+02, 3.7893e+02],\n",
      "        [2.5590e+02, 3.9283e+02, 3.3651e+02, 5.4716e+02],\n",
      "        [2.1723e-01, 1.6965e+02, 4.0952e+01, 2.4641e+02]])\n",
      "xyxyn: tensor([[4.9767e-01, 3.1920e-01, 7.0367e-01, 4.9035e-01],\n",
      "        [2.4994e-01, 4.0209e-01, 4.3713e-01, 5.9208e-01],\n",
      "        [3.9984e-01, 6.1380e-01, 5.2580e-01, 8.5494e-01],\n",
      "        [3.3942e-04, 2.6508e-01, 6.3987e-02, 3.8502e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 82.3ms\n",
      "Speed: 3.4ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8841, 0.8412, 0.8336, 0.7747])\n",
      "data: tensor([[200.3960,  93.9533, 344.4499, 384.2974,   0.8841,   1.0000],\n",
      "        [540.6587,  62.4985, 639.5472, 349.6325,   0.8412,   1.0000],\n",
      "        [343.2307,  90.4381, 445.7552, 380.3339,   0.8336,   1.0000],\n",
      "        [442.6506,  33.1862, 538.5416, 356.0259,   0.7747,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[272.4229, 239.1253, 144.0539, 290.3441],\n",
      "        [590.1030, 206.0655,  98.8885, 287.1340],\n",
      "        [394.4930, 235.3860, 102.5245, 289.8958],\n",
      "        [490.5961, 194.6061,  95.8911, 322.8397]])\n",
      "xywhn: tensor([[0.4257, 0.3736, 0.2251, 0.4537],\n",
      "        [0.9220, 0.3220, 0.1545, 0.4486],\n",
      "        [0.6164, 0.3678, 0.1602, 0.4530],\n",
      "        [0.7666, 0.3041, 0.1498, 0.5044]])\n",
      "xyxy: tensor([[200.3960,  93.9533, 344.4499, 384.2974],\n",
      "        [540.6587,  62.4985, 639.5472, 349.6325],\n",
      "        [343.2307,  90.4381, 445.7552, 380.3339],\n",
      "        [442.6506,  33.1862, 538.5416, 356.0259]])\n",
      "xyxyn: tensor([[0.3131, 0.1468, 0.5382, 0.6005],\n",
      "        [0.8448, 0.0977, 0.9993, 0.5463],\n",
      "        [0.5363, 0.1413, 0.6965, 0.5943],\n",
      "        [0.6916, 0.0519, 0.8415, 0.5563]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 73.7ms\n",
      "Speed: 1.9ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8873])\n",
      "data: tensor([[115.6947, 168.0185, 572.3877, 542.7405,   0.8873,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[344.0412, 355.3795, 456.6930, 374.7220]])\n",
      "xywhn: tensor([[0.5376, 0.5553, 0.7136, 0.5855]])\n",
      "xyxy: tensor([[115.6947, 168.0185, 572.3877, 542.7405]])\n",
      "xyxyn: tensor([[0.1808, 0.2625, 0.8944, 0.8480]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 5 large_buss, 87.0ms\n",
      "Speed: 2.9ms preprocess, 87.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 66.9ms\n",
      "Speed: 1.1ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9318, 0.9133, 0.9020, 0.8934, 0.8117])\n",
      "data: tensor([[6.5949e+01, 7.7854e+01, 1.8907e+02, 1.6375e+02, 9.3185e-01, 1.0000e+00],\n",
      "        [2.5012e+02, 3.6009e+02, 4.3328e+02, 5.6003e+02, 9.1330e-01, 1.0000e+00],\n",
      "        [3.9502e+02, 9.8280e+01, 5.2470e+02, 2.0747e+02, 9.0203e-01, 1.0000e+00],\n",
      "        [1.7249e+01, 2.6314e+02, 1.3005e+02, 3.8586e+02, 8.9340e-01, 1.0000e+00],\n",
      "        [6.8901e-02, 1.7112e+02, 3.8516e+01, 2.5373e+02, 8.1170e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[127.5116, 120.8031, 123.1254,  85.8987],\n",
      "        [341.7006, 460.0590, 183.1675, 199.9320],\n",
      "        [459.8606, 152.8743, 129.6837, 109.1893],\n",
      "        [ 73.6486, 324.4992, 112.8000, 122.7158],\n",
      "        [ 19.2923, 212.4242,  38.4467,  82.6050]])\n",
      "xywhn: tensor([[0.1992, 0.1888, 0.1924, 0.1342],\n",
      "        [0.5339, 0.7188, 0.2862, 0.3124],\n",
      "        [0.7185, 0.2389, 0.2026, 0.1706],\n",
      "        [0.1151, 0.5070, 0.1763, 0.1917],\n",
      "        [0.0301, 0.3319, 0.0601, 0.1291]])\n",
      "xyxy: tensor([[6.5949e+01, 7.7854e+01, 1.8907e+02, 1.6375e+02],\n",
      "        [2.5012e+02, 3.6009e+02, 4.3328e+02, 5.6003e+02],\n",
      "        [3.9502e+02, 9.8280e+01, 5.2470e+02, 2.0747e+02],\n",
      "        [1.7249e+01, 2.6314e+02, 1.3005e+02, 3.8586e+02],\n",
      "        [6.8901e-02, 1.7112e+02, 3.8516e+01, 2.5373e+02]])\n",
      "xyxyn: tensor([[1.0305e-01, 1.2165e-01, 2.9543e-01, 2.5586e-01],\n",
      "        [3.9081e-01, 5.6265e-01, 6.7701e-01, 8.7504e-01],\n",
      "        [6.1722e-01, 1.5356e-01, 8.1985e-01, 3.2417e-01],\n",
      "        [2.6951e-02, 4.1116e-01, 2.0320e-01, 6.0290e-01],\n",
      "        [1.0766e-04, 2.6738e-01, 6.0181e-02, 3.9645e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8760, 0.7366])\n",
      "data: tensor([[246.3315, 259.4075, 398.2275, 366.4801,   0.8760,   1.0000],\n",
      "        [616.3854, 196.8214, 640.0000, 304.0696,   0.7366,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[322.2795, 312.9438, 151.8960, 107.0726],\n",
      "        [628.1927, 250.4455,  23.6146, 107.2482]])\n",
      "xywhn: tensor([[0.5036, 0.4890, 0.2373, 0.1673],\n",
      "        [0.9816, 0.3913, 0.0369, 0.1676]])\n",
      "xyxy: tensor([[246.3315, 259.4075, 398.2275, 366.4801],\n",
      "        [616.3854, 196.8214, 640.0000, 304.0696]])\n",
      "xyxyn: tensor([[0.3849, 0.4053, 0.6222, 0.5726],\n",
      "        [0.9631, 0.3075, 1.0000, 0.4751]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 72.9ms\n",
      "Speed: 2.0ms preprocess, 72.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 67.4ms\n",
      "Speed: 1.2ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9017, 0.8924])\n",
      "data: tensor([[ 98.3234, 126.4906, 469.9908, 416.2542,   0.9017,   1.0000],\n",
      "        [ 60.9516, 401.2137, 512.3669, 640.0000,   0.8924,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[284.1571, 271.3724, 371.6674, 289.7635],\n",
      "        [286.6592, 520.6069, 451.4153, 238.7863]])\n",
      "xywhn: tensor([[0.4440, 0.4240, 0.5807, 0.4528],\n",
      "        [0.4479, 0.8134, 0.7053, 0.3731]])\n",
      "xyxy: tensor([[ 98.3234, 126.4906, 469.9908, 416.2542],\n",
      "        [ 60.9516, 401.2137, 512.3669, 640.0000]])\n",
      "xyxyn: tensor([[0.1536, 0.1976, 0.7344, 0.6504],\n",
      "        [0.0952, 0.6269, 0.8006, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9131, 0.9023, 0.8868])\n",
      "data: tensor([[2.5779e+02, 3.3003e+02, 3.9303e+02, 4.5191e+02, 9.1311e-01, 1.0000e+00],\n",
      "        [1.0149e-01, 1.5912e+02, 8.6665e+01, 2.3246e+02, 9.0229e-01, 1.0000e+00],\n",
      "        [1.1377e+02, 9.6987e+01, 2.0547e+02, 1.5459e+02, 8.8684e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[325.4079, 390.9717, 135.2352, 121.8821],\n",
      "        [ 43.3833, 195.7890,  86.5636,  73.3458],\n",
      "        [159.6199, 125.7865,  91.6969,  57.5996]])\n",
      "xywhn: tensor([[0.5084, 0.6109, 0.2113, 0.1904],\n",
      "        [0.0678, 0.3059, 0.1353, 0.1146],\n",
      "        [0.2494, 0.1965, 0.1433, 0.0900]])\n",
      "xyxy: tensor([[2.5779e+02, 3.3003e+02, 3.9303e+02, 4.5191e+02],\n",
      "        [1.0149e-01, 1.5912e+02, 8.6665e+01, 2.3246e+02],\n",
      "        [1.1377e+02, 9.6987e+01, 2.0547e+02, 1.5459e+02]])\n",
      "xyxyn: tensor([[4.0280e-01, 5.1567e-01, 6.1410e-01, 7.0611e-01],\n",
      "        [1.5857e-04, 2.4862e-01, 1.3541e-01, 3.6322e-01],\n",
      "        [1.7777e-01, 1.5154e-01, 3.2104e-01, 2.4154e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 69.4ms\n",
      "Speed: 1.5ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 68.8ms\n",
      "Speed: 1.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8449, 0.8445, 0.8069, 0.4094])\n",
      "data: tensor([[2.4445e+02, 1.6299e+02, 3.9140e+02, 4.5705e+02, 8.4491e-01, 1.0000e+00],\n",
      "        [9.9728e+01, 1.5610e+02, 2.0944e+02, 3.4752e+02, 8.4447e-01, 1.0000e+00],\n",
      "        [6.1719e+00, 1.5402e+02, 1.0050e+02, 3.7723e+02, 8.0688e-01, 1.0000e+00],\n",
      "        [0.0000e+00, 2.3505e+02, 1.2989e+01, 4.0418e+02, 4.0937e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[317.9246, 310.0190, 146.9575, 294.0672],\n",
      "        [154.5862, 251.8098, 109.7174, 191.4149],\n",
      "        [ 53.3380, 265.6268,  94.3322, 223.2157],\n",
      "        [  6.4945, 319.6155,  12.9889, 169.1335]])\n",
      "xywhn: tensor([[0.4968, 0.4844, 0.2296, 0.4595],\n",
      "        [0.2415, 0.3935, 0.1714, 0.2991],\n",
      "        [0.0833, 0.4150, 0.1474, 0.3488],\n",
      "        [0.0101, 0.4994, 0.0203, 0.2643]])\n",
      "xyxy: tensor([[244.4458, 162.9854, 391.4033, 457.0526],\n",
      "        [ 99.7275, 156.1024, 209.4449, 347.5173],\n",
      "        [  6.1719, 154.0190, 100.5041, 377.2347],\n",
      "        [  0.0000, 235.0487,  12.9889, 404.1823]])\n",
      "xyxyn: tensor([[0.3819, 0.2547, 0.6116, 0.7141],\n",
      "        [0.1558, 0.2439, 0.3273, 0.5430],\n",
      "        [0.0096, 0.2407, 0.1570, 0.5894],\n",
      "        [0.0000, 0.3673, 0.0203, 0.6315]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8916])\n",
      "data: tensor([[ 70.1355, 171.0677, 515.3340, 530.2491,   0.8916,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[292.7347, 350.6584, 445.1985, 359.1814]])\n",
      "xywhn: tensor([[0.4574, 0.5479, 0.6956, 0.5612]])\n",
      "xyxy: tensor([[ 70.1355, 171.0677, 515.3340, 530.2491]])\n",
      "xyxyn: tensor([[0.1096, 0.2673, 0.8052, 0.8285]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 70.8ms\n",
      "Speed: 1.3ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 70.9ms\n",
      "Speed: 1.4ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8454, 0.8418, 0.8369, 0.8266])\n",
      "data: tensor([[ 72.4425, 250.2786, 145.1908, 421.3369,   0.8454,   1.0000],\n",
      "        [146.1202, 228.9427, 238.3469, 394.9759,   0.8418,   1.0000],\n",
      "        [230.9301, 177.7340, 356.4046, 382.2787,   0.8369,   1.0000],\n",
      "        [  0.8458, 276.1132,  73.2998, 448.2549,   0.8266,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[108.8166, 335.8077,  72.7482, 171.0583],\n",
      "        [192.2336, 311.9593,  92.2267, 166.0332],\n",
      "        [293.6673, 280.0063, 125.4745, 204.5446],\n",
      "        [ 37.0728, 362.1841,  72.4540, 172.1417]])\n",
      "xywhn: tensor([[0.1700, 0.5247, 0.1137, 0.2673],\n",
      "        [0.3004, 0.4874, 0.1441, 0.2594],\n",
      "        [0.4589, 0.4375, 0.1961, 0.3196],\n",
      "        [0.0579, 0.5659, 0.1132, 0.2690]])\n",
      "xyxy: tensor([[ 72.4425, 250.2786, 145.1908, 421.3369],\n",
      "        [146.1202, 228.9427, 238.3469, 394.9759],\n",
      "        [230.9301, 177.7340, 356.4046, 382.2787],\n",
      "        [  0.8458, 276.1132,  73.2998, 448.2549]])\n",
      "xyxyn: tensor([[0.1132, 0.3911, 0.2269, 0.6583],\n",
      "        [0.2283, 0.3577, 0.3724, 0.6171],\n",
      "        [0.3608, 0.2777, 0.5569, 0.5973],\n",
      "        [0.0013, 0.4314, 0.1145, 0.7004]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.8434, 0.6986, 0.2679])\n",
      "data: tensor([[3.3634e+02, 1.3582e+02, 4.4686e+02, 5.2506e+02, 8.4335e-01, 1.0000e+00],\n",
      "        [5.8122e+02, 2.0970e+02, 6.3887e+02, 6.2411e+02, 6.9864e-01, 1.0000e+00],\n",
      "        [5.9056e+02, 2.1698e+02, 6.4000e+02, 5.4291e+02, 2.6786e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[391.6031, 330.4410, 110.5225, 389.2416],\n",
      "        [610.0439, 416.9036,  57.6475, 414.4050],\n",
      "        [615.2817, 379.9448,  49.4366, 325.9215]])\n",
      "xywhn: tensor([[0.6119, 0.5163, 0.1727, 0.6082],\n",
      "        [0.9532, 0.6514, 0.0901, 0.6475],\n",
      "        [0.9614, 0.5937, 0.0772, 0.5093]])\n",
      "xyxy: tensor([[336.3419, 135.8202, 446.8643, 525.0618],\n",
      "        [581.2202, 209.7010, 638.8677, 624.1061],\n",
      "        [590.5634, 216.9840, 640.0000, 542.9055]])\n",
      "xyxyn: tensor([[0.5255, 0.2122, 0.6982, 0.8204],\n",
      "        [0.9082, 0.3277, 0.9982, 0.9752],\n",
      "        [0.9228, 0.3390, 1.0000, 0.8483]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 72.9ms\n",
      "Speed: 2.2ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 66.1ms\n",
      "Speed: 1.3ms preprocess, 66.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9172, 0.5121])\n",
      "data: tensor([[9.0184e+01, 1.7037e+02, 4.9037e+02, 5.2358e+02, 9.1721e-01, 1.0000e+00],\n",
      "        [4.8141e+02, 5.4229e+02, 6.3976e+02, 6.3847e+02, 5.1206e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[290.2745, 346.9756, 400.1819, 353.2034],\n",
      "        [560.5862, 590.3795, 158.3494,  96.1765]])\n",
      "xywhn: tensor([[0.4536, 0.5421, 0.6253, 0.5519],\n",
      "        [0.8759, 0.9225, 0.2474, 0.1503]])\n",
      "xyxy: tensor([[ 90.1836, 170.3739, 490.3654, 523.5773],\n",
      "        [481.4116, 542.2913, 639.7609, 638.4678]])\n",
      "xyxyn: tensor([[0.1409, 0.2662, 0.7662, 0.8181],\n",
      "        [0.7522, 0.8473, 0.9996, 0.9976]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8767])\n",
      "data: tensor([[177.9955, 243.5593, 479.9500, 523.3047,   0.8767,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[328.9727, 383.4320, 301.9544, 279.7454]])\n",
      "xywhn: tensor([[0.5140, 0.5991, 0.4718, 0.4371]])\n",
      "xyxy: tensor([[177.9955, 243.5593, 479.9500, 523.3047]])\n",
      "xyxyn: tensor([[0.2781, 0.3806, 0.7499, 0.8177]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 82.0ms\n",
      "Speed: 2.5ms preprocess, 82.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 10 large_buss, 72.0ms\n",
      "Speed: 2.2ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8906])\n",
      "data: tensor([[114.7380, 236.9989, 406.2823, 498.1758,   0.8906,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[260.5101, 367.5873, 291.5443, 261.1769]])\n",
      "xywhn: tensor([[0.4070, 0.5744, 0.4555, 0.4081]])\n",
      "xyxy: tensor([[114.7380, 236.9989, 406.2823, 498.1758]])\n",
      "xyxyn: tensor([[0.1793, 0.3703, 0.6348, 0.7784]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8899, 0.8894, 0.8856, 0.8621, 0.8415, 0.8338, 0.8061, 0.7939, 0.7521, 0.6257])\n",
      "data: tensor([[3.7946e+02, 1.9068e-01, 5.0385e+02, 9.4133e+01, 8.8990e-01, 1.0000e+00],\n",
      "        [5.7663e+01, 2.5318e+02, 1.9184e+02, 5.3270e+02, 8.8936e-01, 1.0000e+00],\n",
      "        [1.4094e+00, 0.0000e+00, 1.4992e+02, 1.1713e+02, 8.8558e-01, 1.0000e+00],\n",
      "        [1.8405e+02, 0.0000e+00, 3.2170e+02, 7.8239e+01, 8.6210e-01, 1.0000e+00],\n",
      "        [3.7200e+02, 2.2345e+02, 4.7468e+02, 4.9331e+02, 8.4150e-01, 1.0000e+00],\n",
      "        [5.9209e+02, 2.2797e+02, 6.3988e+02, 3.9675e+02, 8.3380e-01, 1.0000e+00],\n",
      "        [1.9350e+02, 2.4737e+02, 2.8864e+02, 5.2853e+02, 8.0611e-01, 1.0000e+00],\n",
      "        [4.7280e+02, 2.2037e+02, 5.8260e+02, 4.9540e+02, 7.9394e-01, 1.0000e+00],\n",
      "        [2.9213e+02, 2.2891e+02, 3.7334e+02, 5.0272e+02, 7.5207e-01, 1.0000e+00],\n",
      "        [3.2352e+02, 8.4831e-02, 3.8085e+02, 8.3174e+01, 6.2574e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([10, 6])\n",
      "xywh: tensor([[441.6594,  47.1616, 124.3894,  93.9419],\n",
      "        [124.7500, 392.9415, 134.1736, 279.5151],\n",
      "        [ 75.6638,  58.5668, 148.5088, 117.1337],\n",
      "        [252.8771,  39.1194, 137.6533,  78.2389],\n",
      "        [423.3398, 358.3830, 102.6866, 269.8596],\n",
      "        [615.9834, 312.3601,  47.7917, 168.7896],\n",
      "        [241.0719, 387.9472,  95.1375, 281.1608],\n",
      "        [527.7021, 357.8854, 109.8047, 275.0270],\n",
      "        [332.7342, 365.8138,  81.2051, 273.8076],\n",
      "        [352.1846,  41.6294,  57.3326,  83.0892]])\n",
      "xywhn: tensor([[0.6901, 0.0737, 0.1944, 0.1468],\n",
      "        [0.1949, 0.6140, 0.2096, 0.4367],\n",
      "        [0.1182, 0.0915, 0.2320, 0.1830],\n",
      "        [0.3951, 0.0611, 0.2151, 0.1222],\n",
      "        [0.6615, 0.5600, 0.1604, 0.4217],\n",
      "        [0.9625, 0.4881, 0.0747, 0.2637],\n",
      "        [0.3767, 0.6062, 0.1487, 0.4393],\n",
      "        [0.8245, 0.5592, 0.1716, 0.4297],\n",
      "        [0.5199, 0.5716, 0.1269, 0.4278],\n",
      "        [0.5503, 0.0650, 0.0896, 0.1298]])\n",
      "xyxy: tensor([[3.7946e+02, 1.9068e-01, 5.0385e+02, 9.4133e+01],\n",
      "        [5.7663e+01, 2.5318e+02, 1.9184e+02, 5.3270e+02],\n",
      "        [1.4094e+00, 0.0000e+00, 1.4992e+02, 1.1713e+02],\n",
      "        [1.8405e+02, 0.0000e+00, 3.2170e+02, 7.8239e+01],\n",
      "        [3.7200e+02, 2.2345e+02, 4.7468e+02, 4.9331e+02],\n",
      "        [5.9209e+02, 2.2797e+02, 6.3988e+02, 3.9675e+02],\n",
      "        [1.9350e+02, 2.4737e+02, 2.8864e+02, 5.2853e+02],\n",
      "        [4.7280e+02, 2.2037e+02, 5.8260e+02, 4.9540e+02],\n",
      "        [2.9213e+02, 2.2891e+02, 3.7334e+02, 5.0272e+02],\n",
      "        [3.2352e+02, 8.4831e-02, 3.8085e+02, 8.3174e+01]])\n",
      "xyxyn: tensor([[5.9291e-01, 2.9794e-04, 7.8727e-01, 1.4708e-01],\n",
      "        [9.0099e-02, 3.9560e-01, 2.9975e-01, 8.3234e-01],\n",
      "        [2.2022e-03, 0.0000e+00, 2.3425e-01, 1.8302e-01],\n",
      "        [2.8758e-01, 0.0000e+00, 5.0266e-01, 1.2225e-01],\n",
      "        [5.8124e-01, 3.4915e-01, 7.4169e-01, 7.7080e-01],\n",
      "        [9.2514e-01, 3.5620e-01, 9.9981e-01, 6.1993e-01],\n",
      "        [3.0235e-01, 3.8651e-01, 4.5100e-01, 8.2582e-01],\n",
      "        [7.3875e-01, 3.4433e-01, 9.1032e-01, 7.7406e-01],\n",
      "        [4.5646e-01, 3.5767e-01, 5.8334e-01, 7.8550e-01],\n",
      "        [5.0550e-01, 1.3255e-04, 5.9508e-01, 1.2996e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 66.2ms\n",
      "Speed: 1.2ms preprocess, 66.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8833])\n",
      "data: tensor([[ 75.4481, 161.0892, 546.7603, 555.2641,   0.8833,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[311.1042, 358.1766, 471.3122, 394.1749]])\n",
      "xywhn: tensor([[0.4861, 0.5597, 0.7364, 0.6159]])\n",
      "xyxy: tensor([[ 75.4481, 161.0892, 546.7603, 555.2641]])\n",
      "xyxyn: tensor([[0.1179, 0.2517, 0.8543, 0.8676]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 76.8ms\n",
      "Speed: 2.0ms preprocess, 76.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 large_buss, 68.3ms\n",
      "Speed: 1.3ms preprocess, 68.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9063])\n",
      "data: tensor([[122.9520, 209.0366, 447.9471, 464.2085,   0.9063,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[285.4496, 336.6226, 324.9951, 255.1719]])\n",
      "xywhn: tensor([[0.4460, 0.5260, 0.5078, 0.3987]])\n",
      "xyxy: tensor([[122.9520, 209.0366, 447.9471, 464.2085]])\n",
      "xyxyn: tensor([[0.1921, 0.3266, 0.6999, 0.7253]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.9007, 0.8991, 0.8851, 0.7884])\n",
      "data: tensor([[5.3438e+02, 5.4277e+02, 6.2286e+02, 6.4000e+02, 9.0073e-01, 1.0000e+00],\n",
      "        [2.2215e+00, 2.3168e+02, 9.1400e+01, 3.0630e+02, 8.9912e-01, 1.0000e+00],\n",
      "        [2.6939e-02, 1.2188e+02, 4.5971e+01, 1.7648e+02, 8.8514e-01, 1.0000e+00],\n",
      "        [7.3571e+01, 3.2323e+02, 1.1302e+02, 3.8866e+02, 7.8836e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[578.6183, 591.3862,  88.4785,  97.2277],\n",
      "        [ 46.8106, 268.9924,  89.1782,  74.6245],\n",
      "        [ 22.9990, 149.1777,  45.9441,  54.6046],\n",
      "        [ 93.2937, 355.9466,  39.4447,  65.4365]])\n",
      "xywhn: tensor([[0.9041, 0.9240, 0.1382, 0.1519],\n",
      "        [0.0731, 0.4203, 0.1393, 0.1166],\n",
      "        [0.0359, 0.2331, 0.0718, 0.0853],\n",
      "        [0.1458, 0.5562, 0.0616, 0.1022]])\n",
      "xyxy: tensor([[5.3438e+02, 5.4277e+02, 6.2286e+02, 6.4000e+02],\n",
      "        [2.2215e+00, 2.3168e+02, 9.1400e+01, 3.0630e+02],\n",
      "        [2.6939e-02, 1.2188e+02, 4.5971e+01, 1.7648e+02],\n",
      "        [7.3571e+01, 3.2323e+02, 1.1302e+02, 3.8866e+02]])\n",
      "xyxyn: tensor([[8.3497e-01, 8.4808e-01, 9.7321e-01, 1.0000e+00],\n",
      "        [3.4711e-03, 3.6200e-01, 1.4281e-01, 4.7860e-01],\n",
      "        [4.2093e-05, 1.9043e-01, 7.1830e-02, 2.7575e-01],\n",
      "        [1.1496e-01, 5.0504e-01, 1.7659e-01, 6.0729e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 67.3ms\n",
      "Speed: 1.0ms preprocess, 67.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 65.8ms\n",
      "Speed: 1.3ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9033])\n",
      "data: tensor([[129.3913, 167.1391, 411.2159, 398.9507,   0.9033,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[270.3036, 283.0449, 281.8246, 231.8116]])\n",
      "xywhn: tensor([[0.4223, 0.4423, 0.4404, 0.3622]])\n",
      "xyxy: tensor([[129.3913, 167.1391, 411.2159, 398.9507]])\n",
      "xyxyn: tensor([[0.2022, 0.2612, 0.6425, 0.6234]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8811])\n",
      "data: tensor([[136.0556, 239.4009, 518.9722, 587.8461,   0.8811,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[327.5139, 413.6235, 382.9166, 348.4451]])\n",
      "xywhn: tensor([[0.5117, 0.6463, 0.5983, 0.5444]])\n",
      "xyxy: tensor([[136.0556, 239.4009, 518.9722, 587.8461]])\n",
      "xyxyn: tensor([[0.2126, 0.3741, 0.8109, 0.9185]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 6 large_buss, 73.1ms\n",
      "Speed: 1.8ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 67.7ms\n",
      "Speed: 1.3ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9307, 0.9244, 0.9077, 0.8953, 0.8880, 0.8743])\n",
      "data: tensor([[580.3887, 542.1608, 640.0000, 640.0000,   0.9307,   1.0000],\n",
      "        [562.5958, 379.8319, 640.0000, 482.4238,   0.9244,   1.0000],\n",
      "        [505.0263, 328.5151, 569.7569, 423.3704,   0.9077,   1.0000],\n",
      "        [109.7316, 382.4681, 257.3398, 501.6383,   0.8953,   1.0000],\n",
      "        [453.4548, 326.6553, 503.9001, 421.1241,   0.8880,   1.0000],\n",
      "        [571.8269, 315.2817, 640.0000, 402.1727,   0.8743,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[610.1943, 591.0804,  59.6113,  97.8392],\n",
      "        [601.2979, 431.1279,  77.4042, 102.5919],\n",
      "        [537.3916, 375.9427,  64.7306,  94.8552],\n",
      "        [183.5357, 442.0532, 147.6082, 119.1702],\n",
      "        [478.6775, 373.8897,  50.4453,  94.4688],\n",
      "        [605.9135, 358.7272,  68.1731,  86.8910]])\n",
      "xywhn: tensor([[0.9534, 0.9236, 0.0931, 0.1529],\n",
      "        [0.9395, 0.6736, 0.1209, 0.1603],\n",
      "        [0.8397, 0.5874, 0.1011, 0.1482],\n",
      "        [0.2868, 0.6907, 0.2306, 0.1862],\n",
      "        [0.7479, 0.5842, 0.0788, 0.1476],\n",
      "        [0.9467, 0.5605, 0.1065, 0.1358]])\n",
      "xyxy: tensor([[580.3887, 542.1608, 640.0000, 640.0000],\n",
      "        [562.5958, 379.8319, 640.0000, 482.4238],\n",
      "        [505.0263, 328.5151, 569.7569, 423.3704],\n",
      "        [109.7316, 382.4681, 257.3398, 501.6383],\n",
      "        [453.4548, 326.6553, 503.9001, 421.1241],\n",
      "        [571.8269, 315.2817, 640.0000, 402.1727]])\n",
      "xyxyn: tensor([[0.9069, 0.8471, 1.0000, 1.0000],\n",
      "        [0.8791, 0.5935, 1.0000, 0.7538],\n",
      "        [0.7891, 0.5133, 0.8902, 0.6615],\n",
      "        [0.1715, 0.5976, 0.4021, 0.7838],\n",
      "        [0.7085, 0.5104, 0.7873, 0.6580],\n",
      "        [0.8935, 0.4926, 1.0000, 0.6284]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.8950, 0.8905, 0.8819])\n",
      "data: tensor([[108.9955, 416.1793, 241.8511, 594.4650,   0.8950,   1.0000],\n",
      "        [338.1469, 286.1450, 625.1176, 472.7349,   0.8905,   1.0000],\n",
      "        [ 38.1016, 248.4741, 220.4199, 468.4535,   0.8819,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[175.4233, 505.3221, 132.8556, 178.2856],\n",
      "        [481.6322, 379.4399, 286.9706, 186.5900],\n",
      "        [129.2608, 358.4638, 182.3183, 219.9794]])\n",
      "xywhn: tensor([[0.2741, 0.7896, 0.2076, 0.2786],\n",
      "        [0.7526, 0.5929, 0.4484, 0.2915],\n",
      "        [0.2020, 0.5601, 0.2849, 0.3437]])\n",
      "xyxy: tensor([[108.9955, 416.1793, 241.8511, 594.4650],\n",
      "        [338.1469, 286.1450, 625.1176, 472.7349],\n",
      "        [ 38.1016, 248.4741, 220.4199, 468.4535]])\n",
      "xyxyn: tensor([[0.1703, 0.6503, 0.3779, 0.9289],\n",
      "        [0.5284, 0.4471, 0.9767, 0.7386],\n",
      "        [0.0595, 0.3882, 0.3444, 0.7320]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 77.5ms\n",
      "Speed: 2.3ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 large_buss, 68.9ms\n",
      "Speed: 1.5ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9006])\n",
      "data: tensor([[187.1555, 227.1995, 518.4944, 500.3851,   0.9006,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[352.8250, 363.7923, 331.3389, 273.1856]])\n",
      "xywhn: tensor([[0.5513, 0.5684, 0.5177, 0.4269]])\n",
      "xyxy: tensor([[187.1555, 227.1995, 518.4944, 500.3851]])\n",
      "xyxyn: tensor([[0.2924, 0.3550, 0.8101, 0.7819]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8847, 0.8435, 0.8344, 0.7906])\n",
      "data: tensor([[352.9938,   0.0000, 462.9276, 185.2337,   0.8847,   1.0000],\n",
      "        [459.4484,   0.0000, 541.3644, 186.2029,   0.8435,   1.0000],\n",
      "        [314.6163, 218.6603, 413.9802, 577.3917,   0.8344,   1.0000],\n",
      "        [575.0735,   0.0000, 639.9487, 158.0758,   0.7906,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[407.9607,  92.6169, 109.9338, 185.2337],\n",
      "        [500.4064,  93.1014,  81.9160, 186.2029],\n",
      "        [364.2982, 398.0260,  99.3639, 358.7314],\n",
      "        [607.5111,  79.0379,  64.8752, 158.0758]])\n",
      "xywhn: tensor([[0.6374, 0.1447, 0.1718, 0.2894],\n",
      "        [0.7819, 0.1455, 0.1280, 0.2909],\n",
      "        [0.5692, 0.6219, 0.1553, 0.5605],\n",
      "        [0.9492, 0.1235, 0.1014, 0.2470]])\n",
      "xyxy: tensor([[352.9938,   0.0000, 462.9276, 185.2337],\n",
      "        [459.4484,   0.0000, 541.3644, 186.2029],\n",
      "        [314.6163, 218.6603, 413.9802, 577.3917],\n",
      "        [575.0735,   0.0000, 639.9487, 158.0758]])\n",
      "xyxyn: tensor([[0.5516, 0.0000, 0.7233, 0.2894],\n",
      "        [0.7179, 0.0000, 0.8459, 0.2909],\n",
      "        [0.4916, 0.3417, 0.6468, 0.9022],\n",
      "        [0.8986, 0.0000, 0.9999, 0.2470]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 73.5ms\n",
      "Speed: 2.0ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 71.4ms\n",
      "Speed: 1.4ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9023])\n",
      "data: tensor([[ 79.6858, 204.6856, 466.5587, 476.1458,   0.9023,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[273.1223, 340.4157, 386.8730, 271.4602]])\n",
      "xywhn: tensor([[0.4268, 0.5319, 0.6045, 0.4242]])\n",
      "xyxy: tensor([[ 79.6858, 204.6856, 466.5587, 476.1458]])\n",
      "xyxyn: tensor([[0.1245, 0.3198, 0.7290, 0.7440]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8962])\n",
      "data: tensor([[154.5525, 369.8848, 305.2119, 575.4371,   0.8962,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[229.8822, 472.6609, 150.6594, 205.5522]])\n",
      "xywhn: tensor([[0.3592, 0.7385, 0.2354, 0.3212]])\n",
      "xyxy: tensor([[154.5525, 369.8848, 305.2119, 575.4371]])\n",
      "xyxyn: tensor([[0.2415, 0.5779, 0.4769, 0.8991]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 72.1ms\n",
      "Speed: 1.7ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 large_buss, 69.8ms\n",
      "Speed: 1.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9057])\n",
      "data: tensor([[ 98.3986, 178.2477, 463.2591, 489.8098,   0.9057,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[280.8289, 334.0287, 364.8605, 311.5620]])\n",
      "xywhn: tensor([[0.4388, 0.5219, 0.5701, 0.4868]])\n",
      "xyxy: tensor([[ 98.3986, 178.2477, 463.2591, 489.8098]])\n",
      "xyxyn: tensor([[0.1537, 0.2785, 0.7238, 0.7653]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9172, 0.8760, 0.8744, 0.8723, 0.8609, 0.8577])\n",
      "data: tensor([[6.5856e+01, 4.7980e+02, 3.0100e+02, 6.3431e+02, 9.1717e-01, 1.0000e+00],\n",
      "        [4.5922e+02, 1.7843e+02, 5.6304e+02, 3.9455e+02, 8.7605e-01, 1.0000e+00],\n",
      "        [5.6327e+02, 1.7817e+02, 6.3622e+02, 3.9344e+02, 8.7443e-01, 1.0000e+00],\n",
      "        [5.6395e+02, 0.0000e+00, 6.4000e+02, 5.3932e+01, 8.7232e-01, 1.0000e+00],\n",
      "        [4.4963e+02, 8.4545e-02, 5.6272e+02, 5.3731e+01, 8.6087e-01, 1.0000e+00],\n",
      "        [2.9928e+02, 2.0117e+02, 3.9045e+02, 4.6460e+02, 8.5770e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[183.4288, 557.0549, 235.1454, 154.5115],\n",
      "        [511.1280, 286.4913, 103.8253, 216.1155],\n",
      "        [599.7480, 285.8059,  72.9508, 215.2692],\n",
      "        [601.9738,  26.9658,  76.0524,  53.9315],\n",
      "        [506.1757,  26.9077, 113.0850,  53.6463],\n",
      "        [344.8658, 332.8883,  91.1656, 263.4266]])\n",
      "xywhn: tensor([[0.2866, 0.8704, 0.3674, 0.2414],\n",
      "        [0.7986, 0.4476, 0.1622, 0.3377],\n",
      "        [0.9371, 0.4466, 0.1140, 0.3364],\n",
      "        [0.9406, 0.0421, 0.1188, 0.0843],\n",
      "        [0.7909, 0.0420, 0.1767, 0.0838],\n",
      "        [0.5389, 0.5201, 0.1424, 0.4116]])\n",
      "xyxy: tensor([[6.5856e+01, 4.7980e+02, 3.0100e+02, 6.3431e+02],\n",
      "        [4.5922e+02, 1.7843e+02, 5.6304e+02, 3.9455e+02],\n",
      "        [5.6327e+02, 1.7817e+02, 6.3622e+02, 3.9344e+02],\n",
      "        [5.6395e+02, 0.0000e+00, 6.4000e+02, 5.3932e+01],\n",
      "        [4.4963e+02, 8.4545e-02, 5.6272e+02, 5.3731e+01],\n",
      "        [2.9928e+02, 2.0117e+02, 3.9045e+02, 4.6460e+02]])\n",
      "xyxyn: tensor([[1.0290e-01, 7.4969e-01, 4.7031e-01, 9.9111e-01],\n",
      "        [7.1752e-01, 2.7880e-01, 8.7975e-01, 6.1648e-01],\n",
      "        [8.8011e-01, 2.7839e-01, 9.9410e-01, 6.1475e-01],\n",
      "        [8.8117e-01, 0.0000e+00, 1.0000e+00, 8.4268e-02],\n",
      "        [7.0255e-01, 1.3210e-04, 8.7925e-01, 8.3954e-02],\n",
      "        [4.6763e-01, 3.1434e-01, 6.1008e-01, 7.2594e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 72.0ms\n",
      "Speed: 1.8ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 72.9ms\n",
      "Speed: 1.9ms preprocess, 72.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.9324, 0.8831, 0.8824, 0.8451])\n",
      "data: tensor([[153.0513, 278.8475, 279.9012, 382.7223,   0.9324,   1.0000],\n",
      "        [102.3412,  85.3703, 194.1385, 146.1793,   0.8831,   1.0000],\n",
      "        [  0.0000, 148.7553,  74.5752, 222.7779,   0.8824,   1.0000],\n",
      "        [577.4816, 462.1206, 639.6783, 633.3396,   0.8451,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[216.4763, 330.7849, 126.8500, 103.8748],\n",
      "        [148.2399, 115.7748,  91.7973,  60.8090],\n",
      "        [ 37.2876, 185.7666,  74.5752,  74.0226],\n",
      "        [608.5800, 547.7301,  62.1968, 171.2190]])\n",
      "xywhn: tensor([[0.3382, 0.5169, 0.1982, 0.1623],\n",
      "        [0.2316, 0.1809, 0.1434, 0.0950],\n",
      "        [0.0583, 0.2903, 0.1165, 0.1157],\n",
      "        [0.9509, 0.8558, 0.0972, 0.2675]])\n",
      "xyxy: tensor([[153.0513, 278.8475, 279.9012, 382.7223],\n",
      "        [102.3412,  85.3703, 194.1385, 146.1793],\n",
      "        [  0.0000, 148.7553,  74.5752, 222.7779],\n",
      "        [577.4816, 462.1206, 639.6783, 633.3396]])\n",
      "xyxyn: tensor([[0.2391, 0.4357, 0.4373, 0.5980],\n",
      "        [0.1599, 0.1334, 0.3033, 0.2284],\n",
      "        [0.0000, 0.2324, 0.1165, 0.3481],\n",
      "        [0.9023, 0.7221, 0.9995, 0.9896]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.8949, 0.8922, 0.7972])\n",
      "data: tensor([[355.4653, 304.0687, 577.7875, 425.6843,   0.8949,   1.0000],\n",
      "        [ 42.1740, 271.0739, 184.3740, 453.7516,   0.8922,   1.0000],\n",
      "        [115.2425, 368.3591, 177.9727, 519.4723,   0.7972,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[466.6264, 364.8765, 222.3222, 121.6156],\n",
      "        [113.2740, 362.4127, 142.2000, 182.6777],\n",
      "        [146.6076, 443.9157,  62.7302, 151.1132]])\n",
      "xywhn: tensor([[0.7291, 0.5701, 0.3474, 0.1900],\n",
      "        [0.1770, 0.5663, 0.2222, 0.2854],\n",
      "        [0.2291, 0.6936, 0.0980, 0.2361]])\n",
      "xyxy: tensor([[355.4653, 304.0687, 577.7875, 425.6843],\n",
      "        [ 42.1740, 271.0739, 184.3740, 453.7516],\n",
      "        [115.2425, 368.3591, 177.9727, 519.4723]])\n",
      "xyxyn: tensor([[0.5554, 0.4751, 0.9028, 0.6651],\n",
      "        [0.0659, 0.4236, 0.2881, 0.7090],\n",
      "        [0.1801, 0.5756, 0.2781, 0.8117]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 10 large_buss, 72.0ms\n",
      "Speed: 1.5ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 71.1ms\n",
      "Speed: 1.7ms preprocess, 71.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9001, 0.8979, 0.8962, 0.8913, 0.8854, 0.8832, 0.8749, 0.8601, 0.8333, 0.5834])\n",
      "data: tensor([[8.0875e+01, 2.5255e+02, 2.0636e+02, 5.0861e+02, 9.0009e-01, 1.0000e+00],\n",
      "        [5.8163e+02, 2.2750e+02, 6.4000e+02, 3.9441e+02, 8.9787e-01, 1.0000e+00],\n",
      "        [2.6787e+01, 7.8918e-02, 1.6428e+02, 1.3378e+02, 8.9619e-01, 1.0000e+00],\n",
      "        [3.8382e+02, 0.0000e+00, 4.9755e+02, 1.0983e+02, 8.9130e-01, 1.0000e+00],\n",
      "        [3.3215e+02, 0.0000e+00, 3.8377e+02, 9.8025e+01, 8.8542e-01, 1.0000e+00],\n",
      "        [2.5078e+02, 2.2825e+02, 3.7862e+02, 4.8448e+02, 8.8316e-01, 1.0000e+00],\n",
      "        [2.0260e+02, 4.8195e-02, 3.3426e+02, 9.5868e+01, 8.7488e-01, 1.0000e+00],\n",
      "        [4.7379e+02, 2.2706e+02, 5.7283e+02, 4.7808e+02, 8.6009e-01, 1.0000e+00],\n",
      "        [3.7945e+02, 2.2696e+02, 4.7342e+02, 4.7828e+02, 8.3333e-01, 1.0000e+00],\n",
      "        [8.7677e-02, 1.6637e+01, 2.6563e+01, 1.1542e+02, 5.8337e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([10, 6])\n",
      "xywh: tensor([[143.6168, 380.5808, 125.4833, 256.0606],\n",
      "        [610.8150, 310.9529,  58.3700, 166.9095],\n",
      "        [ 95.5318,  66.9285, 137.4890, 133.6992],\n",
      "        [440.6849,  54.9128, 113.7295, 109.8256],\n",
      "        [357.9595,  49.0123,  51.6216,  98.0246],\n",
      "        [314.7004, 356.3665, 127.8403, 256.2234],\n",
      "        [268.4275,  47.9581, 131.6606,  95.8198],\n",
      "        [523.3134, 352.5715,  99.0408, 251.0163],\n",
      "        [426.4387, 352.6233,  93.9675, 251.3179],\n",
      "        [ 13.3252,  66.0307,  26.4750,  98.7874]])\n",
      "xywhn: tensor([[0.2244, 0.5947, 0.1961, 0.4001],\n",
      "        [0.9544, 0.4859, 0.0912, 0.2608],\n",
      "        [0.1493, 0.1046, 0.2148, 0.2089],\n",
      "        [0.6886, 0.0858, 0.1777, 0.1716],\n",
      "        [0.5593, 0.0766, 0.0807, 0.1532],\n",
      "        [0.4917, 0.5568, 0.1998, 0.4003],\n",
      "        [0.4194, 0.0749, 0.2057, 0.1497],\n",
      "        [0.8177, 0.5509, 0.1548, 0.3922],\n",
      "        [0.6663, 0.5510, 0.1468, 0.3927],\n",
      "        [0.0208, 0.1032, 0.0414, 0.1544]])\n",
      "xyxy: tensor([[8.0875e+01, 2.5255e+02, 2.0636e+02, 5.0861e+02],\n",
      "        [5.8163e+02, 2.2750e+02, 6.4000e+02, 3.9441e+02],\n",
      "        [2.6787e+01, 7.8918e-02, 1.6428e+02, 1.3378e+02],\n",
      "        [3.8382e+02, 0.0000e+00, 4.9755e+02, 1.0983e+02],\n",
      "        [3.3215e+02, 0.0000e+00, 3.8377e+02, 9.8025e+01],\n",
      "        [2.5078e+02, 2.2825e+02, 3.7862e+02, 4.8448e+02],\n",
      "        [2.0260e+02, 4.8195e-02, 3.3426e+02, 9.5868e+01],\n",
      "        [4.7379e+02, 2.2706e+02, 5.7283e+02, 4.7808e+02],\n",
      "        [3.7945e+02, 2.2696e+02, 4.7342e+02, 4.7828e+02],\n",
      "        [8.7677e-02, 1.6637e+01, 2.6563e+01, 1.1542e+02]])\n",
      "xyxyn: tensor([[1.2637e-01, 3.9461e-01, 3.2244e-01, 7.9470e-01],\n",
      "        [9.0880e-01, 3.5547e-01, 1.0000e+00, 6.1626e-01],\n",
      "        [4.1855e-02, 1.2331e-04, 2.5668e-01, 2.0903e-01],\n",
      "        [5.9972e-01, 0.0000e+00, 7.7742e-01, 1.7160e-01],\n",
      "        [5.1898e-01, 0.0000e+00, 5.9964e-01, 1.5316e-01],\n",
      "        [3.9184e-01, 3.5665e-01, 5.9159e-01, 7.5700e-01],\n",
      "        [3.1656e-01, 7.5305e-05, 5.2228e-01, 1.4979e-01],\n",
      "        [7.4030e-01, 3.5479e-01, 8.9505e-01, 7.4700e-01],\n",
      "        [5.9290e-01, 3.5463e-01, 7.3972e-01, 7.4732e-01],\n",
      "        [1.3700e-04, 2.5995e-02, 4.1504e-02, 1.8035e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9076, 0.8987])\n",
      "data: tensor([[421.3250, 500.2956, 640.0000, 639.4395,   0.9076,   1.0000],\n",
      "        [ 66.2657, 280.3331, 372.9588, 468.4844,   0.8987,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[530.6625, 569.8676, 218.6750, 139.1438],\n",
      "        [219.6122, 374.4088, 306.6931, 188.1513]])\n",
      "xywhn: tensor([[0.8292, 0.8904, 0.3417, 0.2174],\n",
      "        [0.3431, 0.5850, 0.4792, 0.2940]])\n",
      "xyxy: tensor([[421.3250, 500.2956, 640.0000, 639.4395],\n",
      "        [ 66.2657, 280.3331, 372.9588, 468.4844]])\n",
      "xyxyn: tensor([[0.6583, 0.7817, 1.0000, 0.9991],\n",
      "        [0.1035, 0.4380, 0.5827, 0.7320]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 large_buss, 70.5ms\n",
      "Speed: 1.4ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 large_buss, 69.2ms\n",
      "Speed: 1.5ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.9166, 0.8859, 0.8078, 0.7975])\n",
      "data: tensor([[114.9292, 306.9482, 430.8731, 481.7297,   0.9166,   1.0000],\n",
      "        [554.4111,  79.7486, 639.9583, 265.6773,   0.8859,   1.0000],\n",
      "        [312.4971,  24.9673, 438.0085, 289.6958,   0.8078,   1.0000],\n",
      "        [436.7170, 115.7245, 554.0425, 283.7786,   0.7975,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[272.9012, 394.3389, 315.9439, 174.7815],\n",
      "        [597.1847, 172.7130,  85.5471, 185.9287],\n",
      "        [375.2528, 157.3315, 125.5114, 264.7285],\n",
      "        [495.3798, 199.7515, 117.3254, 168.0541]])\n",
      "xywhn: tensor([[0.4264, 0.6162, 0.4937, 0.2731],\n",
      "        [0.9331, 0.2699, 0.1337, 0.2905],\n",
      "        [0.5863, 0.2458, 0.1961, 0.4136],\n",
      "        [0.7740, 0.3121, 0.1833, 0.2626]])\n",
      "xyxy: tensor([[114.9292, 306.9482, 430.8731, 481.7297],\n",
      "        [554.4111,  79.7486, 639.9583, 265.6773],\n",
      "        [312.4971,  24.9673, 438.0085, 289.6958],\n",
      "        [436.7170, 115.7245, 554.0425, 283.7786]])\n",
      "xyxyn: tensor([[0.1796, 0.4796, 0.6732, 0.7527],\n",
      "        [0.8663, 0.1246, 0.9999, 0.4151],\n",
      "        [0.4883, 0.0390, 0.6844, 0.4526],\n",
      "        [0.6824, 0.1808, 0.8657, 0.4434]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8902, 0.8358, 0.8049, 0.3000])\n",
      "data: tensor([[3.6374e+02, 3.0786e+02, 5.7500e+02, 4.2219e+02, 8.9021e-01, 1.0000e+00],\n",
      "        [1.0454e+02, 3.5024e+02, 1.8281e+02, 5.1897e+02, 8.3581e-01, 1.0000e+00],\n",
      "        [3.8481e+01, 2.7638e+02, 1.7416e+02, 4.5041e+02, 8.0485e-01, 1.0000e+00],\n",
      "        [4.5634e+02, 5.9467e+02, 5.8489e+02, 6.4000e+02, 3.0004e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[469.3705, 365.0260, 211.2574, 114.3311],\n",
      "        [143.6752, 434.6049,  78.2696, 168.7274],\n",
      "        [106.3228, 363.3934, 135.6833, 174.0237],\n",
      "        [520.6143, 617.3368, 128.5578,  45.3264]])\n",
      "xywhn: tensor([[0.7334, 0.5704, 0.3301, 0.1786],\n",
      "        [0.2245, 0.6791, 0.1223, 0.2636],\n",
      "        [0.1661, 0.5678, 0.2120, 0.2719],\n",
      "        [0.8135, 0.9646, 0.2009, 0.0708]])\n",
      "xyxy: tensor([[363.7418, 307.8605, 574.9993, 422.1915],\n",
      "        [104.5405, 350.2411, 182.8100, 518.9686],\n",
      "        [ 38.4812, 276.3816, 174.1645, 450.4052],\n",
      "        [456.3354, 594.6736, 584.8932, 640.0000]])\n",
      "xyxyn: tensor([[0.5683, 0.4810, 0.8984, 0.6597],\n",
      "        [0.1633, 0.5473, 0.2856, 0.8109],\n",
      "        [0.0601, 0.4318, 0.2721, 0.7038],\n",
      "        [0.7130, 0.9292, 0.9139, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 73.4ms\n",
      "Speed: 1.6ms preprocess, 73.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 67.6ms\n",
      "Speed: 1.4ms preprocess, 67.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9154, 0.8538])\n",
      "data: tensor([[ 68.0933, 169.6068, 481.3306, 515.9539,   0.9154,   1.0000],\n",
      "        [286.2207, 476.2143, 640.0000, 640.0000,   0.8538,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[274.7119, 342.7803, 413.2374, 346.3470],\n",
      "        [463.1104, 558.1072, 353.7793, 163.7857]])\n",
      "xywhn: tensor([[0.4292, 0.5356, 0.6457, 0.5412],\n",
      "        [0.7236, 0.8720, 0.5528, 0.2559]])\n",
      "xyxy: tensor([[ 68.0933, 169.6068, 481.3306, 515.9539],\n",
      "        [286.2207, 476.2143, 640.0000, 640.0000]])\n",
      "xyxyn: tensor([[0.1064, 0.2650, 0.7521, 0.8062],\n",
      "        [0.4472, 0.7441, 1.0000, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8938, 0.8856])\n",
      "data: tensor([[123.9235, 271.9130, 476.3094, 481.6696,   0.8938,   1.0000],\n",
      "        [504.6731,   0.0000, 638.8954, 159.0441,   0.8856,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[300.1165, 376.7913, 352.3859, 209.7566],\n",
      "        [571.7842,  79.5220, 134.2223, 159.0441]])\n",
      "xywhn: tensor([[0.4689, 0.5887, 0.5506, 0.3277],\n",
      "        [0.8934, 0.1243, 0.2097, 0.2485]])\n",
      "xyxy: tensor([[123.9235, 271.9130, 476.3094, 481.6696],\n",
      "        [504.6731,   0.0000, 638.8954, 159.0441]])\n",
      "xyxyn: tensor([[0.1936, 0.4249, 0.7442, 0.7526],\n",
      "        [0.7886, 0.0000, 0.9983, 0.2485]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 large_buss, 70.3ms\n",
      "Speed: 1.6ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9175, 0.8888, 0.8849])\n",
      "data: tensor([[435.5326, 319.5624, 568.1011, 435.5382,   0.9175,   1.0000],\n",
      "        [ 53.6867, 237.7381, 121.8970, 316.6716,   0.8888,   1.0000],\n",
      "        [275.5301, 364.9634, 396.2745, 493.1859,   0.8849,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[501.8168, 377.5503, 132.5685, 115.9758],\n",
      "        [ 87.7918, 277.2048,  68.2103,  78.9335],\n",
      "        [335.9023, 429.0746, 120.7444, 128.2224]])\n",
      "xywhn: tensor([[0.7841, 0.5899, 0.2071, 0.1812],\n",
      "        [0.1372, 0.4331, 0.1066, 0.1233],\n",
      "        [0.5248, 0.6704, 0.1887, 0.2003]])\n",
      "xyxy: tensor([[435.5326, 319.5624, 568.1011, 435.5382],\n",
      "        [ 53.6867, 237.7381, 121.8970, 316.6716],\n",
      "        [275.5301, 364.9634, 396.2745, 493.1859]])\n",
      "xyxyn: tensor([[0.6805, 0.4993, 0.8877, 0.6805],\n",
      "        [0.0839, 0.3715, 0.1905, 0.4948],\n",
      "        [0.4305, 0.5703, 0.6192, 0.7706]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 73.1ms\n",
      "Speed: 1.7ms preprocess, 73.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 8 large_buss, 69.8ms\n",
      "Speed: 1.9ms preprocess, 69.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9030])\n",
      "data: tensor([[118.2791, 252.7215, 419.3383, 449.3112,   0.9030,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[268.8087, 351.0164, 301.0592, 196.5897]])\n",
      "xywhn: tensor([[0.4200, 0.5485, 0.4704, 0.3072]])\n",
      "xyxy: tensor([[118.2791, 252.7215, 419.3383, 449.3112]])\n",
      "xyxyn: tensor([[0.1848, 0.3949, 0.6552, 0.7020]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8848, 0.8818, 0.8417, 0.8163, 0.8060, 0.8038, 0.6228, 0.5156])\n",
      "data: tensor([[7.9598e+01, 0.0000e+00, 2.0991e+02, 1.1281e+02, 8.8476e-01, 1.0000e+00],\n",
      "        [3.5821e+02, 2.2502e+02, 5.0759e+02, 4.8620e+02, 8.8177e-01, 1.0000e+00],\n",
      "        [4.1153e-02, 2.8857e+02, 8.9757e+01, 5.5903e+02, 8.4168e-01, 1.0000e+00],\n",
      "        [8.6743e+01, 2.6677e+02, 2.0509e+02, 5.5182e+02, 8.1627e-01, 1.0000e+00],\n",
      "        [1.7883e+01, 0.0000e+00, 7.9725e+01, 1.0497e+02, 8.0602e-01, 1.0000e+00],\n",
      "        [2.0231e+02, 2.7461e+02, 3.0659e+02, 5.3290e+02, 8.0385e-01, 1.0000e+00],\n",
      "        [3.0481e+02, 2.5936e+02, 3.5685e+02, 4.5007e+02, 6.2281e-01, 1.0000e+00],\n",
      "        [2.4448e-02, 2.7314e-01, 1.7008e+01, 4.4320e+01, 5.1559e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[144.7535,  56.4066, 130.3104, 112.8132],\n",
      "        [432.9014, 355.6109, 149.3845, 261.1824],\n",
      "        [ 44.8993, 423.7960,  89.7162, 270.4613],\n",
      "        [145.9169, 409.2970, 118.3468, 285.0482],\n",
      "        [ 48.8042,  52.4828,  61.8416, 104.9657],\n",
      "        [254.4498, 403.7523, 104.2759, 258.2908],\n",
      "        [330.8328, 354.7175,  52.0405, 190.7078],\n",
      "        [  8.5163,  22.2965,  16.9836,  44.0467]])\n",
      "xywhn: tensor([[0.2262, 0.0881, 0.2036, 0.1763],\n",
      "        [0.6764, 0.5556, 0.2334, 0.4081],\n",
      "        [0.0702, 0.6622, 0.1402, 0.4226],\n",
      "        [0.2280, 0.6395, 0.1849, 0.4454],\n",
      "        [0.0763, 0.0820, 0.0966, 0.1640],\n",
      "        [0.3976, 0.6309, 0.1629, 0.4036],\n",
      "        [0.5169, 0.5542, 0.0813, 0.2980],\n",
      "        [0.0133, 0.0348, 0.0265, 0.0688]])\n",
      "xyxy: tensor([[7.9598e+01, 0.0000e+00, 2.0991e+02, 1.1281e+02],\n",
      "        [3.5821e+02, 2.2502e+02, 5.0759e+02, 4.8620e+02],\n",
      "        [4.1153e-02, 2.8857e+02, 8.9757e+01, 5.5903e+02],\n",
      "        [8.6743e+01, 2.6677e+02, 2.0509e+02, 5.5182e+02],\n",
      "        [1.7883e+01, 0.0000e+00, 7.9725e+01, 1.0497e+02],\n",
      "        [2.0231e+02, 2.7461e+02, 3.0659e+02, 5.3290e+02],\n",
      "        [3.0481e+02, 2.5936e+02, 3.5685e+02, 4.5007e+02],\n",
      "        [2.4448e-02, 2.7314e-01, 1.7008e+01, 4.4320e+01]])\n",
      "xyxyn: tensor([[1.2437e-01, 0.0000e+00, 3.2798e-01, 1.7627e-01],\n",
      "        [5.5970e-01, 3.5159e-01, 7.9312e-01, 7.5969e-01],\n",
      "        [6.4301e-05, 4.5088e-01, 1.4025e-01, 8.7348e-01],\n",
      "        [1.3554e-01, 4.1683e-01, 3.2045e-01, 8.6222e-01],\n",
      "        [2.7943e-02, 0.0000e+00, 1.2457e-01, 1.6401e-01],\n",
      "        [3.1611e-01, 4.2907e-01, 4.7904e-01, 8.3265e-01],\n",
      "        [4.7627e-01, 4.0526e-01, 5.5758e-01, 7.0324e-01],\n",
      "        [3.8201e-05, 4.2679e-04, 2.6575e-02, 6.9250e-02]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 68.0ms\n",
      "Speed: 1.6ms preprocess, 68.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 68.2ms\n",
      "Speed: 1.7ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8906, 0.8768])\n",
      "data: tensor([[250.0830, 279.5085, 476.4027, 584.9443,   0.8906,   1.0000],\n",
      "        [ 88.7282, 163.8070, 326.6186, 461.3890,   0.8768,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[363.2429, 432.2264, 226.3197, 305.4358],\n",
      "        [207.6734, 312.5980, 237.8904, 297.5820]])\n",
      "xywhn: tensor([[0.5676, 0.6754, 0.3536, 0.4772],\n",
      "        [0.3245, 0.4884, 0.3717, 0.4650]])\n",
      "xyxy: tensor([[250.0830, 279.5085, 476.4027, 584.9443],\n",
      "        [ 88.7282, 163.8070, 326.6186, 461.3890]])\n",
      "xyxyn: tensor([[0.3908, 0.4367, 0.7444, 0.9140],\n",
      "        [0.1386, 0.2559, 0.5103, 0.7209]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9077, 0.9043, 0.8861])\n",
      "data: tensor([[2.8986e+02, 3.3884e+02, 4.2690e+02, 4.6173e+02, 9.0773e-01, 1.0000e+00],\n",
      "        [1.1225e+02, 9.1585e+01, 2.0350e+02, 1.5031e+02, 9.0428e-01, 1.0000e+00],\n",
      "        [1.3716e-01, 1.5337e+02, 8.6203e+01, 2.3082e+02, 8.8615e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[358.3810, 400.2851, 137.0467, 122.8989],\n",
      "        [157.8730, 120.9491,  91.2488,  58.7290],\n",
      "        [ 43.1699, 192.0941,  86.0654,  77.4477]])\n",
      "xywhn: tensor([[0.5600, 0.6254, 0.2141, 0.1920],\n",
      "        [0.2467, 0.1890, 0.1426, 0.0918],\n",
      "        [0.0675, 0.3001, 0.1345, 0.1210]])\n",
      "xyxy: tensor([[2.8986e+02, 3.3884e+02, 4.2690e+02, 4.6173e+02],\n",
      "        [1.1225e+02, 9.1585e+01, 2.0350e+02, 1.5031e+02],\n",
      "        [1.3716e-01, 1.5337e+02, 8.6203e+01, 2.3082e+02]])\n",
      "xyxyn: tensor([[4.5290e-01, 5.2943e-01, 6.6704e-01, 7.2146e-01],\n",
      "        [1.7539e-01, 1.4310e-01, 3.1796e-01, 2.3487e-01],\n",
      "        [2.1431e-04, 2.3964e-01, 1.3469e-01, 3.6065e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 large_buss, 66.4ms\n",
      "Speed: 1.6ms preprocess, 66.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 71.0ms\n",
      "Speed: 1.5ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9100, 0.5915, 0.4333])\n",
      "data: tensor([[9.8970e+01, 2.5038e+02, 2.6391e+02, 4.0711e+02, 9.1001e-01, 1.0000e+00],\n",
      "        [0.0000e+00, 7.9515e+01, 1.5845e+01, 1.8275e+02, 5.9154e-01, 1.0000e+00],\n",
      "        [5.3690e-02, 1.6960e+02, 2.2624e+01, 2.2615e+02, 4.3333e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[181.4415, 328.7450, 164.9425, 156.7210],\n",
      "        [  7.9224, 131.1341,  15.8448, 103.2383],\n",
      "        [ 11.3390, 197.8735,  22.5706,  56.5461]])\n",
      "xywhn: tensor([[0.2835, 0.5137, 0.2577, 0.2449],\n",
      "        [0.0124, 0.2049, 0.0248, 0.1613],\n",
      "        [0.0177, 0.3092, 0.0353, 0.0884]])\n",
      "xyxy: tensor([[9.8970e+01, 2.5038e+02, 2.6391e+02, 4.0711e+02],\n",
      "        [0.0000e+00, 7.9515e+01, 1.5845e+01, 1.8275e+02],\n",
      "        [5.3690e-02, 1.6960e+02, 2.2624e+01, 2.2615e+02]])\n",
      "xyxyn: tensor([[1.5464e-01, 3.9123e-01, 4.1236e-01, 6.3610e-01],\n",
      "        [0.0000e+00, 1.2424e-01, 2.4758e-02, 2.8555e-01],\n",
      "        [8.3891e-05, 2.6500e-01, 3.5350e-02, 3.5335e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9124])\n",
      "data: tensor([[ 99.9249, 197.4755, 475.1452, 515.9810,   0.9124,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[287.5351, 356.7282, 375.2203, 318.5054]])\n",
      "xywhn: tensor([[0.4493, 0.5574, 0.5863, 0.4977]])\n",
      "xyxy: tensor([[ 99.9249, 197.4755, 475.1452, 515.9810]])\n",
      "xyxyn: tensor([[0.1561, 0.3086, 0.7424, 0.8062]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 66.7ms\n",
      "Speed: 1.4ms preprocess, 66.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 72.6ms\n",
      "Speed: 1.6ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9145])\n",
      "data: tensor([[ 98.2176, 194.6897, 480.0125, 516.4719,   0.9145,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[289.1150, 355.5807, 381.7949, 321.7822]])\n",
      "xywhn: tensor([[0.4517, 0.5556, 0.5966, 0.5028]])\n",
      "xyxy: tensor([[ 98.2176, 194.6897, 480.0125, 516.4719]])\n",
      "xyxyn: tensor([[0.1535, 0.3042, 0.7500, 0.8070]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8865, 0.8650])\n",
      "data: tensor([[1.5041e-01, 2.1684e+02, 7.1854e+01, 3.0289e+02, 8.8652e-01, 1.0000e+00],\n",
      "        [5.3084e+01, 3.1875e+02, 1.3856e+02, 4.1185e+02, 8.6503e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 36.0020, 259.8668,  71.7032,  86.0490],\n",
      "        [ 95.8241, 365.2984,  85.4810,  93.0940]])\n",
      "xywhn: tensor([[0.0563, 0.4060, 0.1120, 0.1345],\n",
      "        [0.1497, 0.5708, 0.1336, 0.1455]])\n",
      "xyxy: tensor([[1.5041e-01, 2.1684e+02, 7.1854e+01, 3.0289e+02],\n",
      "        [5.3084e+01, 3.1875e+02, 1.3856e+02, 4.1185e+02]])\n",
      "xyxyn: tensor([[2.3501e-04, 3.3882e-01, 1.1227e-01, 4.7327e-01],\n",
      "        [8.2943e-02, 4.9805e-01, 2.1651e-01, 6.4351e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 68.5ms\n",
      "Speed: 1.1ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 65.4ms\n",
      "Speed: 1.1ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8961])\n",
      "data: tensor([[100.9049,  78.3848, 385.8511, 404.2274,   0.8961,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[243.3780, 241.3061, 284.9462, 325.8426]])\n",
      "xywhn: tensor([[0.3803, 0.3770, 0.4452, 0.5091]])\n",
      "xyxy: tensor([[100.9049,  78.3848, 385.8511, 404.2274]])\n",
      "xyxyn: tensor([[0.1577, 0.1225, 0.6029, 0.6316]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8916, 0.8767])\n",
      "data: tensor([[141.6244, 283.1945, 452.6885, 462.8550,   0.8916,   1.0000],\n",
      "        [521.0370,  18.9137, 640.0000, 275.3888,   0.8767,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[297.1565, 373.0247, 311.0641, 179.6605],\n",
      "        [580.5185, 147.1512, 118.9630, 256.4751]])\n",
      "xywhn: tensor([[0.4643, 0.5829, 0.4860, 0.2807],\n",
      "        [0.9071, 0.2299, 0.1859, 0.4007]])\n",
      "xyxy: tensor([[141.6244, 283.1945, 452.6885, 462.8550],\n",
      "        [521.0370,  18.9137, 640.0000, 275.3888]])\n",
      "xyxyn: tensor([[0.2213, 0.4425, 0.7073, 0.7232],\n",
      "        [0.8141, 0.0296, 1.0000, 0.4303]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 64.5ms\n",
      "Speed: 1.2ms preprocess, 64.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 large_buss, 64.3ms\n",
      "Speed: 1.1ms preprocess, 64.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8674])\n",
      "data: tensor([[ 73.3136, 170.9837, 519.6682, 546.3219,   0.8674,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[296.4909, 358.6528, 446.3546, 375.3382]])\n",
      "xywhn: tensor([[0.4633, 0.5604, 0.6974, 0.5865]])\n",
      "xyxy: tensor([[ 73.3136, 170.9837, 519.6682, 546.3219]])\n",
      "xyxyn: tensor([[0.1146, 0.2672, 0.8120, 0.8536]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9228, 0.8947, 0.8800])\n",
      "data: tensor([[312.8719, 378.5182, 437.1882, 502.9018,   0.9228,   1.0000],\n",
      "        [ 26.2864, 207.8178, 109.3310, 281.4402,   0.8947,   1.0000],\n",
      "        [132.5496, 126.3369, 212.1575, 186.3929,   0.8800,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[375.0301, 440.7100, 124.3162, 124.3835],\n",
      "        [ 67.8087, 244.6290,  83.0446,  73.6224],\n",
      "        [172.3536, 156.3649,  79.6079,  60.0560]])\n",
      "xywhn: tensor([[0.5860, 0.6886, 0.1942, 0.1943],\n",
      "        [0.1060, 0.3822, 0.1298, 0.1150],\n",
      "        [0.2693, 0.2443, 0.1244, 0.0938]])\n",
      "xyxy: tensor([[312.8719, 378.5182, 437.1882, 502.9018],\n",
      "        [ 26.2864, 207.8178, 109.3310, 281.4402],\n",
      "        [132.5496, 126.3369, 212.1575, 186.3929]])\n",
      "xyxyn: tensor([[0.4889, 0.5914, 0.6831, 0.7858],\n",
      "        [0.0411, 0.3247, 0.1708, 0.4398],\n",
      "        [0.2071, 0.1974, 0.3315, 0.2912]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 82.3ms\n",
      "Speed: 3.5ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9045, 0.8860])\n",
      "data: tensor([[ 62.7476, 159.4921, 478.7682, 516.1658,   0.9045,   1.0000],\n",
      "        [223.3165, 475.5540, 639.0657, 639.9580,   0.8860,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[270.7579, 337.8290, 416.0206, 356.6737],\n",
      "        [431.1911, 557.7560, 415.7492, 164.4040]])\n",
      "xywhn: tensor([[0.4231, 0.5279, 0.6500, 0.5573],\n",
      "        [0.6737, 0.8715, 0.6496, 0.2569]])\n",
      "xyxy: tensor([[ 62.7476, 159.4921, 478.7682, 516.1658],\n",
      "        [223.3165, 475.5540, 639.0657, 639.9580]])\n",
      "xyxyn: tensor([[0.0980, 0.2492, 0.7481, 0.8065],\n",
      "        [0.3489, 0.7431, 0.9985, 0.9999]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 large_buss, 92.1ms\n",
      "Speed: 4.9ms preprocess, 92.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9036, 0.8985, 0.8945])\n",
      "data: tensor([[ 62.1852, 325.6706, 150.6616, 407.3508,   0.9036,   1.0000],\n",
      "        [279.1786, 195.3084, 373.1765, 267.8618,   0.8985,   1.0000],\n",
      "        [  0.0000, 247.2175,  78.3089, 321.5655,   0.8945,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[106.4234, 366.5107,  88.4764,  81.6802],\n",
      "        [326.1775, 231.5851,  93.9979,  72.5533],\n",
      "        [ 39.1544, 284.3915,  78.3089,  74.3480]])\n",
      "xywhn: tensor([[0.1663, 0.5727, 0.1382, 0.1276],\n",
      "        [0.5097, 0.3619, 0.1469, 0.1134],\n",
      "        [0.0612, 0.4444, 0.1224, 0.1162]])\n",
      "xyxy: tensor([[ 62.1852, 325.6706, 150.6616, 407.3508],\n",
      "        [279.1786, 195.3084, 373.1765, 267.8618],\n",
      "        [  0.0000, 247.2175,  78.3089, 321.5655]])\n",
      "xyxyn: tensor([[0.0972, 0.5089, 0.2354, 0.6365],\n",
      "        [0.4362, 0.3052, 0.5831, 0.4185],\n",
      "        [0.0000, 0.3863, 0.1224, 0.5024]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 large_buss, 94.8ms\n",
      "Speed: 3.2ms preprocess, 94.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9049, 0.8503, 0.7614])\n",
      "data: tensor([[1.3698e+02, 2.4235e+02, 4.1702e+02, 4.4089e+02, 9.0487e-01, 1.0000e+00],\n",
      "        [4.8244e+02, 0.0000e+00, 5.9341e+02, 1.5744e+02, 8.5029e-01, 1.0000e+00],\n",
      "        [5.9303e+02, 2.7390e-01, 6.3986e+02, 1.4723e+02, 7.6142e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[277.0031, 341.6214, 280.0416, 198.5450],\n",
      "        [537.9228,  78.7194, 110.9712, 157.4387],\n",
      "        [616.4426,  73.7506,  46.8296, 146.9534]])\n",
      "xywhn: tensor([[0.4328, 0.5338, 0.4376, 0.3102],\n",
      "        [0.8405, 0.1230, 0.1734, 0.2460],\n",
      "        [0.9632, 0.1152, 0.0732, 0.2296]])\n",
      "xyxy: tensor([[1.3698e+02, 2.4235e+02, 4.1702e+02, 4.4089e+02],\n",
      "        [4.8244e+02, 0.0000e+00, 5.9341e+02, 1.5744e+02],\n",
      "        [5.9303e+02, 2.7390e-01, 6.3986e+02, 1.4723e+02]])\n",
      "xyxyn: tensor([[2.1403e-01, 3.7867e-01, 6.5160e-01, 6.8890e-01],\n",
      "        [7.5381e-01, 0.0000e+00, 9.2720e-01, 2.4600e-01],\n",
      "        [9.2661e-01, 4.2796e-04, 9.9978e-01, 2.3004e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 96.5ms\n",
      "Speed: 3.9ms preprocess, 96.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8997, 0.8547])\n",
      "data: tensor([[201.1864, 406.8322, 478.8212, 609.5214,   0.8997,   1.0000],\n",
      "        [ 38.6632, 243.6689, 295.2602, 390.7192,   0.8547,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[340.0038, 508.1768, 277.6348, 202.6893],\n",
      "        [166.9617, 317.1941, 256.5970, 147.0503]])\n",
      "xywhn: tensor([[0.5313, 0.7940, 0.4338, 0.3167],\n",
      "        [0.2609, 0.4956, 0.4009, 0.2298]])\n",
      "xyxy: tensor([[201.1864, 406.8322, 478.8212, 609.5214],\n",
      "        [ 38.6632, 243.6689, 295.2602, 390.7192]])\n",
      "xyxyn: tensor([[0.3144, 0.6357, 0.7482, 0.9524],\n",
      "        [0.0604, 0.3807, 0.4613, 0.6105]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 9 large_buss, 87.2ms\n",
      "Speed: 4.4ms preprocess, 87.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8842, 0.8654, 0.8587, 0.8572, 0.8564, 0.8460, 0.8175, 0.8157, 0.7795])\n",
      "data: tensor([[1.2763e+02, 3.4091e-01, 2.4102e+02, 1.4092e+02, 8.8420e-01, 1.0000e+00],\n",
      "        [3.2167e-01, 6.4613e-01, 7.3582e+01, 1.3364e+02, 8.6541e-01, 1.0000e+00],\n",
      "        [2.3610e+02, 2.8234e+02, 3.2340e+02, 5.0341e+02, 8.5874e-01, 1.0000e+00],\n",
      "        [7.5565e+01, 1.0743e+01, 1.2627e+02, 1.3716e+02, 8.5719e-01, 1.0000e+00],\n",
      "        [3.8014e+02, 2.7335e+02, 4.9578e+02, 4.6560e+02, 8.5638e-01, 1.0000e+00],\n",
      "        [5.5508e+01, 2.9161e+02, 1.3308e+02, 5.2934e+02, 8.4602e-01, 1.0000e+00],\n",
      "        [1.3337e+02, 2.8154e+02, 2.3423e+02, 5.2450e+02, 8.1754e-01, 1.0000e+00],\n",
      "        [3.2299e+02, 2.8883e+02, 3.7826e+02, 4.5107e+02, 8.1569e-01, 1.0000e+00],\n",
      "        [2.5467e-02, 2.7516e+02, 5.5666e+01, 5.6654e+02, 7.7952e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([9, 6])\n",
      "xywh: tensor([[184.3284,  70.6307, 113.3928, 140.5797],\n",
      "        [ 36.9516,  67.1455,  73.2599, 132.9986],\n",
      "        [279.7460, 392.8763,  87.3007, 221.0736],\n",
      "        [100.9172,  73.9505,  50.7053, 126.4142],\n",
      "        [437.9609, 369.4774, 115.6345, 192.2457],\n",
      "        [ 94.2920, 410.4744,  77.5677, 237.7217],\n",
      "        [183.7971, 403.0226, 100.8623, 242.9556],\n",
      "        [350.6270, 369.9495,  55.2663, 162.2385],\n",
      "        [ 27.8458, 420.8488,  55.6407, 291.3749]])\n",
      "xywhn: tensor([[0.2880, 0.1104, 0.1772, 0.2197],\n",
      "        [0.0577, 0.1049, 0.1145, 0.2078],\n",
      "        [0.4371, 0.6139, 0.1364, 0.3454],\n",
      "        [0.1577, 0.1155, 0.0792, 0.1975],\n",
      "        [0.6843, 0.5773, 0.1807, 0.3004],\n",
      "        [0.1473, 0.6414, 0.1212, 0.3714],\n",
      "        [0.2872, 0.6297, 0.1576, 0.3796],\n",
      "        [0.5479, 0.5780, 0.0864, 0.2535],\n",
      "        [0.0435, 0.6576, 0.0869, 0.4553]])\n",
      "xyxy: tensor([[1.2763e+02, 3.4091e-01, 2.4102e+02, 1.4092e+02],\n",
      "        [3.2167e-01, 6.4613e-01, 7.3582e+01, 1.3364e+02],\n",
      "        [2.3610e+02, 2.8234e+02, 3.2340e+02, 5.0341e+02],\n",
      "        [7.5565e+01, 1.0743e+01, 1.2627e+02, 1.3716e+02],\n",
      "        [3.8014e+02, 2.7335e+02, 4.9578e+02, 4.6560e+02],\n",
      "        [5.5508e+01, 2.9161e+02, 1.3308e+02, 5.2934e+02],\n",
      "        [1.3337e+02, 2.8154e+02, 2.3423e+02, 5.2450e+02],\n",
      "        [3.2299e+02, 2.8883e+02, 3.7826e+02, 4.5107e+02],\n",
      "        [2.5467e-02, 2.7516e+02, 5.5666e+01, 5.6654e+02]])\n",
      "xyxyn: tensor([[1.9942e-01, 5.3267e-04, 3.7660e-01, 2.2019e-01],\n",
      "        [5.0261e-04, 1.0096e-03, 1.1497e-01, 2.0882e-01],\n",
      "        [3.6890e-01, 4.4116e-01, 5.0531e-01, 7.8658e-01],\n",
      "        [1.1807e-01, 1.6787e-02, 1.9730e-01, 2.1431e-01],\n",
      "        [5.9397e-01, 4.2712e-01, 7.7465e-01, 7.2750e-01],\n",
      "        [8.6731e-02, 4.5565e-01, 2.0793e-01, 8.2709e-01],\n",
      "        [2.0838e-01, 4.3991e-01, 3.6598e-01, 8.1953e-01],\n",
      "        [5.0468e-01, 4.5130e-01, 5.9103e-01, 7.0479e-01],\n",
      "        [3.9792e-05, 4.2994e-01, 8.6978e-02, 8.8521e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 86.3ms\n",
      "Speed: 2.3ms preprocess, 86.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8843, 0.7963])\n",
      "data: tensor([[141.3804, 288.2612, 450.0330, 463.1085,   0.8843,   1.0000],\n",
      "        [577.7899,  42.4777, 639.2342, 281.4311,   0.7963,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[295.7067, 375.6849, 308.6526, 174.8473],\n",
      "        [608.5120, 161.9544,  61.4443, 238.9534]])\n",
      "xywhn: tensor([[0.4620, 0.5870, 0.4823, 0.2732],\n",
      "        [0.9508, 0.2531, 0.0960, 0.3734]])\n",
      "xyxy: tensor([[141.3804, 288.2612, 450.0330, 463.1085],\n",
      "        [577.7899,  42.4777, 639.2342, 281.4311]])\n",
      "xyxyn: tensor([[0.2209, 0.4504, 0.7032, 0.7236],\n",
      "        [0.9028, 0.0664, 0.9988, 0.4397]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 5 large_buss, 102.2ms\n",
      "Speed: 4.2ms preprocess, 102.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9032, 0.8986, 0.7873, 0.7635, 0.2525])\n",
      "data: tensor([[1.0105e+02, 3.2260e+02, 4.2222e+02, 5.2587e+02, 9.0321e-01, 1.0000e+00],\n",
      "        [4.3555e+02, 6.0182e+01, 6.3965e+02, 2.9252e+02, 8.9864e-01, 1.0000e+00],\n",
      "        [1.9326e+02, 5.3251e+01, 3.1800e+02, 3.1299e+02, 7.8731e-01, 1.0000e+00],\n",
      "        [3.1903e+02, 1.3135e+02, 4.3397e+02, 2.9989e+02, 7.6349e-01, 1.0000e+00],\n",
      "        [3.7004e+02, 5.7825e+02, 3.9767e+02, 6.4000e+02, 2.5253e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[261.6359, 424.2370, 321.1621, 203.2712],\n",
      "        [537.6010, 176.3501, 204.0975, 232.3357],\n",
      "        [255.6295, 183.1201, 124.7437, 259.7386],\n",
      "        [376.4965, 215.6196, 114.9388, 168.5486],\n",
      "        [383.8531, 609.1270,  27.6356,  61.7460]])\n",
      "xywhn: tensor([[0.4088, 0.6629, 0.5018, 0.3176],\n",
      "        [0.8400, 0.2755, 0.3189, 0.3630],\n",
      "        [0.3994, 0.2861, 0.1949, 0.4058],\n",
      "        [0.5883, 0.3369, 0.1796, 0.2634],\n",
      "        [0.5998, 0.9518, 0.0432, 0.0965]])\n",
      "xyxy: tensor([[101.0549, 322.6014, 422.2170, 525.8726],\n",
      "        [435.5522,  60.1823, 639.6497, 292.5180],\n",
      "        [193.2576,  53.2508, 318.0013, 312.9894],\n",
      "        [319.0271, 131.3453, 433.9659, 299.8939],\n",
      "        [370.0354, 578.2540, 397.6709, 640.0000]])\n",
      "xyxyn: tensor([[0.1579, 0.5041, 0.6597, 0.8217],\n",
      "        [0.6806, 0.0940, 0.9995, 0.4571],\n",
      "        [0.3020, 0.0832, 0.4969, 0.4890],\n",
      "        [0.4985, 0.2052, 0.6781, 0.4686],\n",
      "        [0.5782, 0.9035, 0.6214, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 large_bus, 85.7ms\n",
      "Speed: 2.7ms preprocess, 85.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.7243])\n",
      "data: tensor([[196.4045, 432.4467, 422.9553, 626.0751,   0.7243,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[309.6799, 529.2609, 226.5507, 193.6284]])\n",
      "xywhn: tensor([[0.4839, 0.8270, 0.3540, 0.3025]])\n",
      "xyxy: tensor([[196.4045, 432.4467, 422.9553, 626.0751]])\n",
      "xyxyn: tensor([[0.3069, 0.6757, 0.6609, 0.9782]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 85.0ms\n",
      "Speed: 4.0ms preprocess, 85.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 67.1ms\n",
      "Speed: 1.3ms preprocess, 67.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8976, 0.8747])\n",
      "data: tensor([[ 18.9040, 290.1714, 182.6646, 541.8776,   0.8976,   1.0000],\n",
      "        [255.3763, 258.6159, 533.4103, 410.8810,   0.8747,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[100.7843, 416.0245, 163.7606, 251.7062],\n",
      "        [394.3933, 334.7485, 278.0340, 152.2651]])\n",
      "xywhn: tensor([[0.1575, 0.6500, 0.2559, 0.3933],\n",
      "        [0.6162, 0.5230, 0.4344, 0.2379]])\n",
      "xyxy: tensor([[ 18.9040, 290.1714, 182.6646, 541.8776],\n",
      "        [255.3763, 258.6159, 533.4103, 410.8810]])\n",
      "xyxyn: tensor([[0.0295, 0.4534, 0.2854, 0.8467],\n",
      "        [0.3990, 0.4041, 0.8335, 0.6420]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8759, 0.3531])\n",
      "data: tensor([[7.5431e+01, 1.6101e+02, 5.2464e+02, 5.7260e+02, 8.7589e-01, 1.0000e+00],\n",
      "        [2.9258e+02, 5.1475e+02, 6.3622e+02, 6.4000e+02, 3.5309e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[300.0356, 366.8049, 449.2094, 411.5995],\n",
      "        [464.3999, 577.3727, 343.6389, 125.2546]])\n",
      "xywhn: tensor([[0.4688, 0.5731, 0.7019, 0.6431],\n",
      "        [0.7256, 0.9021, 0.5369, 0.1957]])\n",
      "xyxy: tensor([[ 75.4310, 161.0051, 524.6403, 572.6046],\n",
      "        [292.5805, 514.7454, 636.2194, 640.0000]])\n",
      "xyxyn: tensor([[0.1179, 0.2516, 0.8198, 0.8947],\n",
      "        [0.4572, 0.8043, 0.9941, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 11 large_buss, 66.8ms\n",
      "Speed: 1.7ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 11 large_buss, 64.6ms\n",
      "Speed: 1.2ms preprocess, 64.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9180, 0.9000, 0.8979, 0.8902, 0.8894, 0.8720, 0.8557, 0.8469, 0.8395, 0.8381, 0.8354])\n",
      "data: tensor([[1.7674e+01, 3.2957e+02, 1.0997e+02, 5.0870e+02, 9.1804e-01, 1.0000e+00],\n",
      "        [1.1322e-01, 1.1681e+02, 9.0011e+01, 2.2371e+02, 8.9996e-01, 1.0000e+00],\n",
      "        [9.5073e+01, 1.0590e+02, 1.8670e+02, 2.0846e+02, 8.9793e-01, 1.0000e+00],\n",
      "        [3.6114e+02, 3.2059e+02, 4.1061e+02, 4.4641e+02, 8.9018e-01, 1.0000e+00],\n",
      "        [2.2523e+02, 1.0950e+02, 3.0411e+02, 2.0954e+02, 8.8945e-01, 1.0000e+00],\n",
      "        [4.1358e+02, 3.1868e+02, 4.8011e+02, 4.4528e+02, 8.7197e-01, 1.0000e+00],\n",
      "        [1.0741e+02, 3.2137e+02, 1.7344e+02, 5.0560e+02, 8.5570e-01, 1.0000e+00],\n",
      "        [2.2810e+02, 3.0826e+02, 2.9804e+02, 4.7309e+02, 8.4688e-01, 1.0000e+00],\n",
      "        [2.9815e+02, 3.1414e+02, 3.6143e+02, 4.6727e+02, 8.3947e-01, 1.0000e+00],\n",
      "        [1.7267e+02, 3.1675e+02, 2.2712e+02, 4.8163e+02, 8.3810e-01, 1.0000e+00],\n",
      "        [1.8919e+02, 1.1293e+02, 2.2430e+02, 2.0543e+02, 8.3545e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([11, 6])\n",
      "xywh: tensor([[ 63.8225, 419.1305,  92.2963, 179.1302],\n",
      "        [ 45.0621, 170.2609,  89.8978, 106.8973],\n",
      "        [140.8870, 157.1783,  91.6279, 102.5636],\n",
      "        [385.8743, 383.4998,  49.4630, 125.8260],\n",
      "        [264.6660, 159.5213,  78.8799, 100.0412],\n",
      "        [446.8484, 381.9814,  66.5281, 126.6071],\n",
      "        [140.4245, 413.4845,  66.0334, 184.2242],\n",
      "        [263.0709, 390.6713,  69.9336, 164.8308],\n",
      "        [329.7915, 390.7067,  63.2859, 153.1296],\n",
      "        [199.8937, 399.1888,  54.4465, 164.8807],\n",
      "        [206.7455, 159.1812,  35.1176,  92.5003]])\n",
      "xywhn: tensor([[0.0997, 0.6549, 0.1442, 0.2799],\n",
      "        [0.0704, 0.2660, 0.1405, 0.1670],\n",
      "        [0.2201, 0.2456, 0.1432, 0.1603],\n",
      "        [0.6029, 0.5992, 0.0773, 0.1966],\n",
      "        [0.4135, 0.2493, 0.1232, 0.1563],\n",
      "        [0.6982, 0.5968, 0.1040, 0.1978],\n",
      "        [0.2194, 0.6461, 0.1032, 0.2879],\n",
      "        [0.4110, 0.6104, 0.1093, 0.2575],\n",
      "        [0.5153, 0.6105, 0.0989, 0.2393],\n",
      "        [0.3123, 0.6237, 0.0851, 0.2576],\n",
      "        [0.3230, 0.2487, 0.0549, 0.1445]])\n",
      "xyxy: tensor([[1.7674e+01, 3.2957e+02, 1.0997e+02, 5.0870e+02],\n",
      "        [1.1322e-01, 1.1681e+02, 9.0011e+01, 2.2371e+02],\n",
      "        [9.5073e+01, 1.0590e+02, 1.8670e+02, 2.0846e+02],\n",
      "        [3.6114e+02, 3.2059e+02, 4.1061e+02, 4.4641e+02],\n",
      "        [2.2523e+02, 1.0950e+02, 3.0411e+02, 2.0954e+02],\n",
      "        [4.1358e+02, 3.1868e+02, 4.8011e+02, 4.4528e+02],\n",
      "        [1.0741e+02, 3.2137e+02, 1.7344e+02, 5.0560e+02],\n",
      "        [2.2810e+02, 3.0826e+02, 2.9804e+02, 4.7309e+02],\n",
      "        [2.9815e+02, 3.1414e+02, 3.6143e+02, 4.6727e+02],\n",
      "        [1.7267e+02, 3.1675e+02, 2.2712e+02, 4.8163e+02],\n",
      "        [1.8919e+02, 1.1293e+02, 2.2430e+02, 2.0543e+02]])\n",
      "xyxyn: tensor([[2.7616e-02, 5.1495e-01, 1.7183e-01, 7.9484e-01],\n",
      "        [1.7691e-04, 1.8252e-01, 1.4064e-01, 3.4955e-01],\n",
      "        [1.4855e-01, 1.6546e-01, 2.9172e-01, 3.2572e-01],\n",
      "        [5.6429e-01, 5.0092e-01, 6.4157e-01, 6.9752e-01],\n",
      "        [3.5192e-01, 1.7109e-01, 4.7517e-01, 3.2741e-01],\n",
      "        [6.4623e-01, 4.9793e-01, 7.5018e-01, 6.9576e-01],\n",
      "        [1.6782e-01, 5.0214e-01, 2.7100e-01, 7.8999e-01],\n",
      "        [3.5641e-01, 4.8165e-01, 4.6568e-01, 7.3920e-01],\n",
      "        [4.6586e-01, 4.9085e-01, 5.6474e-01, 7.3011e-01],\n",
      "        [2.6980e-01, 4.9492e-01, 3.5487e-01, 7.5255e-01],\n",
      "        [2.9560e-01, 1.7645e-01, 3.5048e-01, 3.2099e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8965, 0.8826, 0.8335, 0.8311, 0.8272, 0.8030, 0.7964, 0.7831, 0.7809, 0.7731, 0.3725])\n",
      "data: tensor([[1.1835e-01, 0.0000e+00, 7.0044e+01, 1.2800e+02, 8.9647e-01, 1.0000e+00],\n",
      "        [6.1096e+01, 2.0807e+02, 1.9555e+02, 4.8859e+02, 8.8264e-01, 1.0000e+00],\n",
      "        [1.9663e+02, 2.0433e+02, 2.9414e+02, 4.8255e+02, 8.3353e-01, 1.0000e+00],\n",
      "        [3.8327e+02, 1.3042e-01, 4.8771e+02, 5.2906e+01, 8.3113e-01, 1.0000e+00],\n",
      "        [3.2787e+02, 5.3562e-02, 3.8570e+02, 3.9692e+01, 8.2722e-01, 1.0000e+00],\n",
      "        [1.8924e+02, 4.4537e-02, 2.8345e+02, 3.5044e+01, 8.0301e-01, 1.0000e+00],\n",
      "        [2.9490e+02, 1.8068e+02, 3.7602e+02, 4.5698e+02, 7.9637e-01, 1.0000e+00],\n",
      "        [5.9518e+02, 1.7935e+02, 6.3992e+02, 3.4387e+02, 7.8312e-01, 1.0000e+00],\n",
      "        [3.7730e+02, 1.7556e+02, 4.7932e+02, 4.4933e+02, 7.8093e-01, 1.0000e+00],\n",
      "        [4.7967e+02, 1.6500e+02, 5.8655e+02, 4.4871e+02, 7.7310e-01, 1.0000e+00],\n",
      "        [1.8674e+02, 0.0000e+00, 3.2959e+02, 3.7016e+01, 3.7246e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([11, 6])\n",
      "xywh: tensor([[ 35.0813,  64.0018,  69.9259, 128.0035],\n",
      "        [128.3205, 348.3317, 134.4493, 280.5210],\n",
      "        [245.3859, 343.4404,  97.5113, 278.2140],\n",
      "        [435.4898,  26.5183, 104.4454,  52.7757],\n",
      "        [356.7854,  19.8726,  57.8383,  39.6381],\n",
      "        [236.3412,  17.5445,  94.2107,  34.9999],\n",
      "        [335.4581, 318.8289,  81.1191, 276.2971],\n",
      "        [617.5492, 261.6120,  44.7411, 164.5215],\n",
      "        [428.3133, 312.4454, 102.0192, 273.7783],\n",
      "        [533.1082, 306.8528, 106.8854, 283.7146],\n",
      "        [258.1615,  18.5079, 142.8485,  37.0157]])\n",
      "xywhn: tensor([[0.0548, 0.1000, 0.1093, 0.2000],\n",
      "        [0.2005, 0.5443, 0.2101, 0.4383],\n",
      "        [0.3834, 0.5366, 0.1524, 0.4347],\n",
      "        [0.6805, 0.0414, 0.1632, 0.0825],\n",
      "        [0.5575, 0.0311, 0.0904, 0.0619],\n",
      "        [0.3693, 0.0274, 0.1472, 0.0547],\n",
      "        [0.5242, 0.4982, 0.1267, 0.4317],\n",
      "        [0.9649, 0.4088, 0.0699, 0.2571],\n",
      "        [0.6692, 0.4882, 0.1594, 0.4278],\n",
      "        [0.8330, 0.4795, 0.1670, 0.4433],\n",
      "        [0.4034, 0.0289, 0.2232, 0.0578]])\n",
      "xyxy: tensor([[1.1835e-01, 0.0000e+00, 7.0044e+01, 1.2800e+02],\n",
      "        [6.1096e+01, 2.0807e+02, 1.9555e+02, 4.8859e+02],\n",
      "        [1.9663e+02, 2.0433e+02, 2.9414e+02, 4.8255e+02],\n",
      "        [3.8327e+02, 1.3042e-01, 4.8771e+02, 5.2906e+01],\n",
      "        [3.2787e+02, 5.3562e-02, 3.8570e+02, 3.9692e+01],\n",
      "        [1.8924e+02, 4.4537e-02, 2.8345e+02, 3.5044e+01],\n",
      "        [2.9490e+02, 1.8068e+02, 3.7602e+02, 4.5698e+02],\n",
      "        [5.9518e+02, 1.7935e+02, 6.3992e+02, 3.4387e+02],\n",
      "        [3.7730e+02, 1.7556e+02, 4.7932e+02, 4.4933e+02],\n",
      "        [4.7967e+02, 1.6500e+02, 5.8655e+02, 4.4871e+02],\n",
      "        [1.8674e+02, 0.0000e+00, 3.2959e+02, 3.7016e+01]])\n",
      "xyxyn: tensor([[1.8492e-04, 0.0000e+00, 1.0944e-01, 2.0001e-01],\n",
      "        [9.5462e-02, 3.2511e-01, 3.0554e-01, 7.6343e-01],\n",
      "        [3.0723e-01, 3.1927e-01, 4.5960e-01, 7.5398e-01],\n",
      "        [5.9885e-01, 2.0379e-04, 7.6205e-01, 8.2666e-02],\n",
      "        [5.1229e-01, 8.3691e-05, 6.0266e-01, 6.2018e-02],\n",
      "        [2.9568e-01, 6.9588e-05, 4.4289e-01, 5.4757e-02],\n",
      "        [4.6078e-01, 2.8231e-01, 5.8753e-01, 7.1403e-01],\n",
      "        [9.2997e-01, 2.8024e-01, 9.9987e-01, 5.3730e-01],\n",
      "        [5.8954e-01, 2.7431e-01, 7.4894e-01, 7.0209e-01],\n",
      "        [7.4948e-01, 2.5781e-01, 9.1649e-01, 7.0111e-01],\n",
      "        [2.9178e-01, 0.0000e+00, 5.1498e-01, 5.7837e-02]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 3 large_buss, 65.0ms\n",
      "Speed: 1.2ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 66.2ms\n",
      "Speed: 1.0ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.9180, 0.8939, 0.8858])\n",
      "data: tensor([[393.4644, 422.7305, 525.8881, 566.1611,   0.9180,   1.0000],\n",
      "        [ 68.6291, 118.1020, 147.2710, 175.5543,   0.8939,   1.0000],\n",
      "        [ 22.6846, 208.5158, 106.0347, 283.6267,   0.8858,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[459.6762, 494.4458, 132.4238, 143.4305],\n",
      "        [107.9501, 146.8282,  78.6420,  57.4523],\n",
      "        [ 64.3596, 246.0713,  83.3501,  75.1109]])\n",
      "xywhn: tensor([[0.7182, 0.7726, 0.2069, 0.2241],\n",
      "        [0.1687, 0.2294, 0.1229, 0.0898],\n",
      "        [0.1006, 0.3845, 0.1302, 0.1174]])\n",
      "xyxy: tensor([[393.4644, 422.7305, 525.8881, 566.1611],\n",
      "        [ 68.6291, 118.1020, 147.2710, 175.5543],\n",
      "        [ 22.6846, 208.5158, 106.0347, 283.6267]])\n",
      "xyxyn: tensor([[0.6148, 0.6605, 0.8217, 0.8846],\n",
      "        [0.1072, 0.1845, 0.2301, 0.2743],\n",
      "        [0.0354, 0.3258, 0.1657, 0.4432]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8927])\n",
      "data: tensor([[222.9029, 113.5348, 469.0396, 512.0138,   0.8927,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[345.9713, 312.7743, 246.1367, 398.4790]])\n",
      "xywhn: tensor([[0.5406, 0.4887, 0.3846, 0.6226]])\n",
      "xyxy: tensor([[222.9029, 113.5348, 469.0396, 512.0138]])\n",
      "xyxyn: tensor([[0.3483, 0.1774, 0.7329, 0.8000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 62.8ms\n",
      "Speed: 1.2ms preprocess, 62.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 65.5ms\n",
      "Speed: 1.1ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 66.9ms\n",
      "Speed: 1.3ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9008])\n",
      "data: tensor([[242.9322, 228.4994, 467.8341, 506.7104,   0.9008,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[355.3831, 367.6049, 224.9019, 278.2111]])\n",
      "xywhn: tensor([[0.5553, 0.5744, 0.3514, 0.4347]])\n",
      "xyxy: tensor([[242.9322, 228.4994, 467.8341, 506.7104]])\n",
      "xyxyn: tensor([[0.3796, 0.3570, 0.7310, 0.7917]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9051, 0.9047])\n",
      "data: tensor([[160.0877, 243.5459, 365.8075, 434.8067,   0.9051,   1.0000],\n",
      "        [ 72.7077, 346.7059, 139.7667, 484.1769,   0.9047,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[262.9476, 339.1763, 205.7198, 191.2607],\n",
      "        [106.2372, 415.4414,  67.0590, 137.4711]])\n",
      "xywhn: tensor([[0.4109, 0.5300, 0.3214, 0.2988],\n",
      "        [0.1660, 0.6491, 0.1048, 0.2148]])\n",
      "xyxy: tensor([[160.0877, 243.5459, 365.8075, 434.8067],\n",
      "        [ 72.7077, 346.7059, 139.7667, 484.1769]])\n",
      "xyxyn: tensor([[0.2501, 0.3805, 0.5716, 0.6794],\n",
      "        [0.1136, 0.5417, 0.2184, 0.7565]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 large_buss, 67.5ms\n",
      "Speed: 1.2ms preprocess, 67.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.8846])\n",
      "data: tensor([[215.1115, 224.7993, 514.2195, 491.8813,   0.8846,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[364.6655, 358.3403, 299.1080, 267.0820]])\n",
      "xywhn: tensor([[0.5698, 0.5599, 0.4674, 0.4173]])\n",
      "xyxy: tensor([[215.1115, 224.7993, 514.2195, 491.8813]])\n",
      "xyxyn: tensor([[0.3361, 0.3512, 0.8035, 0.7686]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.9033, 0.8925])\n",
      "data: tensor([[481.6828,  67.9766, 638.8679, 538.1474,   0.9033,   1.0000],\n",
      "        [161.0806, 133.8330, 435.2845, 585.1836,   0.8925,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[560.2753, 303.0620, 157.1851, 470.1708],\n",
      "        [298.1826, 359.5083, 274.2039, 451.3506]])\n",
      "xywhn: tensor([[0.8754, 0.4735, 0.2456, 0.7346],\n",
      "        [0.4659, 0.5617, 0.4284, 0.7052]])\n",
      "xyxy: tensor([[481.6828,  67.9766, 638.8679, 538.1474],\n",
      "        [161.0806, 133.8330, 435.2845, 585.1836]])\n",
      "xyxyn: tensor([[0.7526, 0.1062, 0.9982, 0.8409],\n",
      "        [0.2517, 0.2091, 0.6801, 0.9143]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 large_bus, 67.4ms\n",
      "Speed: 1.1ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 large_buss, 64.4ms\n",
      "Speed: 1.0ms preprocess, 64.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9018])\n",
      "data: tensor([[155.4787, 277.2880, 364.4680, 521.3195,   0.9018,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[259.9733, 399.3038, 208.9893, 244.0315]])\n",
      "xywhn: tensor([[0.4062, 0.6239, 0.3265, 0.3813]])\n",
      "xyxy: tensor([[155.4787, 277.2880, 364.4680, 521.3195]])\n",
      "xyxyn: tensor([[0.2429, 0.4333, 0.5695, 0.8146]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8782, 0.8722])\n",
      "data: tensor([[ 44.9645, 298.0424, 177.0363, 419.4987,   0.8782,   1.0000],\n",
      "        [  0.0000, 172.9402,  50.3080, 272.4214,   0.8722,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[111.0004, 358.7706, 132.0718, 121.4563],\n",
      "        [ 25.1540, 222.6808,  50.3080,  99.4811]])\n",
      "xywhn: tensor([[0.1734, 0.5606, 0.2064, 0.1898],\n",
      "        [0.0393, 0.3479, 0.0786, 0.1554]])\n",
      "xyxy: tensor([[ 44.9645, 298.0424, 177.0363, 419.4987],\n",
      "        [  0.0000, 172.9402,  50.3080, 272.4214]])\n",
      "xyxyn: tensor([[0.0703, 0.4657, 0.2766, 0.6555],\n",
      "        [0.0000, 0.2702, 0.0786, 0.4257]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 4 large_buss, 67.2ms\n",
      "Speed: 1.2ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.9048, 0.8999, 0.6763, 0.3418])\n",
      "data: tensor([[3.0072e+02, 2.7510e+02, 5.2275e+02, 4.2408e+02, 9.0477e-01, 1.0000e+00],\n",
      "        [5.6836e+01, 2.7546e+02, 2.2725e+02, 4.5710e+02, 8.9990e-01, 1.0000e+00],\n",
      "        [0.0000e+00, 3.6942e+02, 2.5767e+01, 4.8065e+02, 6.7629e-01, 1.0000e+00],\n",
      "        [0.0000e+00, 3.1759e+02, 2.2143e+01, 4.7899e+02, 3.4181e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[411.7355, 349.5886, 222.0375, 148.9762],\n",
      "        [142.0429, 366.2791, 170.4131, 181.6418],\n",
      "        [ 12.8834, 425.0360,  25.7667, 111.2263],\n",
      "        [ 11.0714, 398.2881,  22.1428, 161.4040]])\n",
      "xywhn: tensor([[0.6433, 0.5462, 0.3469, 0.2328],\n",
      "        [0.2219, 0.5723, 0.2663, 0.2838],\n",
      "        [0.0201, 0.6641, 0.0403, 0.1738],\n",
      "        [0.0173, 0.6223, 0.0346, 0.2522]])\n",
      "xyxy: tensor([[300.7168, 275.1005, 522.7543, 424.0767],\n",
      "        [ 56.8363, 275.4582, 227.2495, 457.1000],\n",
      "        [  0.0000, 369.4229,  25.7667, 480.6492],\n",
      "        [  0.0000, 317.5861,  22.1428, 478.9901]])\n",
      "xyxyn: tensor([[0.4699, 0.4298, 0.8168, 0.6626],\n",
      "        [0.0888, 0.4304, 0.3551, 0.7142],\n",
      "        [0.0000, 0.5772, 0.0403, 0.7510],\n",
      "        [0.0000, 0.4962, 0.0346, 0.7484]])\n"
     ]
    }
   ],
   "source": [
    "for it in pathlib.Path(\"data/car_bus_truck_Detection_2/test/images\").iterdir():\n",
    "    frame = cv2.imread(str(it))\n",
    "\n",
    "    frame = predict_normal(frame)\n",
    "\n",
    "    cv2.imshow(\"tst.jpeg\", frame)\n",
    "\n",
    "    cv2.waitKey(2000) & 0xFF == ord('q')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac2bf64-2805-4d84-83da-20f0159baef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_poly(img, pts):\n",
    "    \n",
    "    mask = np.ones(img.shape[:2], np.uint8) * 255\n",
    "    cv2.drawContours(mask, [pts], -1, (0, 0, 0), -1, cv2.LINE_AA)\n",
    "    \n",
    "    ## (3) do bit-op\n",
    "    dst = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    return dst\n",
    "\n",
    "def crop_poly_white(img, pts):\n",
    "    \n",
    "    mask = np.ones(img.shape[:2], np.uint8) * 255\n",
    "    cv2.drawContours(mask, [pts], -1, (0,0,0), -1, cv2.LINE_AA)\n",
    "    \n",
    "    ## (3) do bit-op\n",
    "    dst = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    ## (4) add the white background\n",
    "    bg = np.ones_like(img, np.uint8)*255\n",
    "    cv2.bitwise_not(bg,bg, mask=mask)\n",
    "    dst2 = bg+ dst\n",
    "    \n",
    "    return dst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2da2de-0108-4804-ba04-8a6b90c0a68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/Users/ameyakore/Downloads/production_id_4979728 (1080p).mp4')\n",
    "count = 0\n",
    "crop_h = 0\n",
    "scale_factor = 1\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    frame = cv2.resize(frame, (640 * scale_factor, 640 * scale_factor))\n",
    "    frame = frame[crop_h * scale_factor:,:]\n",
    "\n",
    "    #Segmentation starts\n",
    "\n",
    "    #For top not necessary if top is cropped\n",
    "    frame = crop_poly_white(frame, np.array([[0, 0], [0, 317], [640, 317], [640, 0]]))\n",
    "    \n",
    "    # For center \n",
    "    frame = crop_poly_white(frame, np.array([[277 * scale_factor, (380 - crop_h) * scale_factor],[410 * scale_factor, (392 - crop_h)* scale_factor],[381* scale_factor, (506 - crop_h)* scale_factor],[155* scale_factor, (498 - crop_h)* scale_factor]]))\n",
    "\n",
    "    #For left side\n",
    "    frame = crop_poly_white(frame, np.array([[97* scale_factor, 0],[97* scale_factor, (365 - crop_h)* scale_factor],[160* scale_factor, (390 - crop_h)* scale_factor],[0, (462 - crop_h)* scale_factor], [0, 0]]))\n",
    "\n",
    "    #For right side\n",
    "    frame = crop_poly_white(frame, np.array([[556* scale_factor, 0],[556* scale_factor, (365 - crop_h)* scale_factor],[504* scale_factor, (370 - crop_h)* scale_factor],[542* scale_factor, (444 - crop_h)* scale_factor], [640* scale_factor, (467 - crop_h)* scale_factor], [640* scale_factor, 0]]))\n",
    "    \n",
    "    # blank_image_mask = np.zeros((640,640,3), np.uint8)\n",
    "    #frame = predict_and_draw_on_frame(frame, ((640 - crop_h)* scale_factor, 640* scale_factor))\n",
    "    # frame = predict_and_draw_on_mask(frame, blank_image_mask)\n",
    "    cv2.imshow('video', frame)\n",
    "    #cv2.imwrite(\"frame%d.jpg\" % count, frame)\n",
    "    count = count + 1\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb36f39-ab3c-4e79-8dfb-88b7f82f66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_profile(path, crop_h, scale_factor=1):\n",
    "    timings = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        model.predict(frame, imgsz = ((640 - crop_h)* scale_factor, 640* scale_factor))\n",
    "        # frame = predict_and_draw_on_frame(frame, ((640 - crop_h)* scale_factor, 640* scale_factor))\n",
    "        t2 = time.time()\n",
    "        timings.append(t2 - t1)\n",
    "        # cv2.imshow('video', frame)\n",
    "        # cv2.imwrite(\"frame%d.jpg\" % count, frame)\n",
    "        count = count + 1\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() # destroy all opened windows\n",
    "\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefdc59-fa10-4689-b84e-fd7e6e21a929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timings = time_profile('moviecrop.mp4', 317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0da8b2-ffa7-475f-9e89-d548b0330260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timings2 = time_profile('movienocrop.mp4', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0831bfc-2df8-45e6-b5fc-d48612265b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timings3 = time_profile('movienocroponlywhite.mp4', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac128e0-d852-4734-8250-e82dc6c438bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.ylim(0, 0.15)\n",
    "plt.plot(timings, label=\"crop and whited out\")\n",
    "plt.plot(timings2, label=\"no processing\")\n",
    "plt.plot(timings3, label=\"whited out but no crop\")\n",
    "plt.legend()\n",
    "print(np.average(timings))\n",
    "print(np.average(timings2))\n",
    "print(np.average(timings3))\n",
    "df = pd.DataFrame()\n",
    "df['crop and whited out'] = timings\n",
    "df['no processing'] = timings2\n",
    "df['whited out but no crop'] = timings3\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118fa52-bc71-4258-96d5-ba7237a073e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict('/Users/ameyakore/Downloads/production_id_4979728 (1080p).mp4', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0b9fc1d-edaa-47b7-be42-e4cf794cb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Run Algo for processing\n",
    "def calculate_pre(rois):\n",
    "    # find bounding rect of all rois\n",
    "    rects = [cv2.boundingRect(r) for r in rois]\n",
    "    dims = math.ceil(math.sqrt(len(rects)))\n",
    "\n",
    "    # Now find the mappings\n",
    "    # return a list of source_x, source_y, dest_x, dest_y, w, h\n",
    "    ans = []\n",
    "    x_cur = 0\n",
    "    y_cur = 0\n",
    "\n",
    "    w_max = 0\n",
    "    h_max = 0\n",
    "    adsroi = []\n",
    "    for idx, (rect, roi) in enumerate(zip(rects, rois)):\n",
    "        # find rows to remove from image\n",
    "        idx += 1\n",
    "        x, y, w, h = rect\n",
    "\n",
    "        off_x = x - x_cur\n",
    "        off_y = y  - y_cur\n",
    "        \n",
    "        ans.append([x, y, x_cur, y_cur, w, h])\n",
    "\n",
    "        adsroi.append(np.array([(x - off_x, y - off_y) for x, y, in roi]))\n",
    "        \n",
    "        x_cur += w\n",
    "\n",
    "        if w_max < x_cur:\n",
    "            w_max = x_cur\n",
    "\n",
    "        if h_max < y_cur + h:\n",
    "            h_max = y_cur + h\n",
    "        \n",
    "        if (dims != 1 and idx % dims == 0) or (dims == 1 and idx == dims):\n",
    "            y_cur += h\n",
    "            x_cur = 0\n",
    "        \n",
    "    return ans, (w_max, h_max), adsroi\n",
    "\n",
    "def pre_process(image, ans, w, h):\n",
    "    new_img = np.ones((h,w,3), np.uint8) * 255\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "def draw_contours(new_img, rois):\n",
    "    new_img_2 = np.ones(new_img.shape, np.uint8) * 255\n",
    "    mask_0 = np.ones(new_img.shape[:-1], np.uint8)\n",
    "    for roi in rois:\n",
    "        mask = np.ones(new_img.shape[:-1], np.uint8)\n",
    "        cv2.drawContours(mask, [roi], -1, 0, -1, cv2.LINE_AA)\n",
    "        cv2.drawContours(new_img_2, [roi], -1, (0, 0, 0), -1, cv2.LINE_AA)\n",
    "        mask_0 += mask\n",
    "\n",
    "       \n",
    "    new_img_2 = cv2.bitwise_or(new_img, new_img_2, mask=mask_0)\n",
    "    return new_img_2\n",
    "\n",
    "\n",
    "def pre_processroi(image, rois, ans, w, h):\n",
    "    new_img = np.ones((h,w,3), np.uint8) * 255\n",
    "    for source_x, source_y, dest_x, dest_y, w, h in ans:\n",
    "        new_img[dest_y: dest_y + h, dest_x: dest_x + w] = image[source_y:source_y + h, source_x : source_x+ w]\n",
    "    \n",
    "    return draw_contours(new_img, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42462aa9-dd3f-443c-8981-54ed9ba798ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_profile_other_no_crop(path, ans, rois, imgsz = (640, 640)):\n",
    "    timings = []\n",
    "    processings = []\n",
    "    print(imgsz)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    old_frame = None\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        #frame = pre_process(frame, ans, imgsz[0], imgsz[1])\n",
    "        frame = draw_contours(frame, rois)\n",
    "        #frame = pre_processroi(frame, rois, ans, imgsz[0], imgsz[1])\n",
    "        #print(frame.shape[:2])\n",
    "        t2 = time.time()\n",
    "        timings.append(t2 - t1)\n",
    "        old_frame = frame\n",
    "        t1 = time.time()\n",
    "        model.predict(frame, imgsz=imgsz, show=False)\n",
    "        # frame = predict_and_draw_on_frame(frame, ((640 - crop_h)* scale_factor, 640* scale_factor))\n",
    "        t2 = time.time()\n",
    "        processings.append(t2 - t1)\n",
    "        #cv2.imshow('video', frame)\n",
    "        # cv2.imwrite(\"frame%d.jpg\" % count, frame)\n",
    "        count = count + 1\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cv2.imwrite(\"framenocrop.jpg\", old_frame)        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() # destroy all opened windows\n",
    "\n",
    "    return timings, processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e487e165-d15c-423d-af19-53b462c4d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_profile_other_crop(path, ans, rois, imgsz = (640, 640)):\n",
    "    timings = []\n",
    "    processings = []\n",
    "    print(imgsz)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    old_frame = None\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        #frame = pre_process(frame, ans, imgsz[0], imgsz[1])\n",
    "        #frame = draw_contours(frame, rois)\n",
    "        frame = pre_processroi(frame, rois, ans, imgsz[0], imgsz[1])\n",
    "        #print(frame.shape[:2])\n",
    "        t2 = time.time()\n",
    "        timings.append(t2 - t1)\n",
    "        old_frame = frame\n",
    "        t1 = time.time()\n",
    "        model.predict(frame, imgsz=imgsz, show=False)\n",
    "        # frame = predict_and_draw_on_frame(frame, ((640 - crop_h)* scale_factor, 640* scale_factor))\n",
    "        t2 = time.time()\n",
    "        processings.append(t2 - t1)\n",
    "        #cv2.imshow('video', frame)\n",
    "        # cv2.imwrite(\"frame%d.jpg\" % count, frame)\n",
    "        count = count + 1\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cv2.imwrite(\"frameoccrop.jpg\", old_frame)    \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() # destroy all opened windows\n",
    "\n",
    "    return timings, processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b79617c6-ef32-4d23-8f46-30a884d7a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = 'movienocrop.mp4'\n",
    "# rois = [np.array([[0, 0], [0, 317], [640, 317], [640, 0]]),\n",
    "  #              np.array([[277 * scale_factor, (380 - crop_h) * scale_factor],[410 * scale_factor, (392 - crop_h)* scale_factor],[381* scale_factor, (506 - crop_h)* scale_factor],[155* scale_factor, (498 - crop_h)* scale_factor]]),\n",
    "   #             np.array([[97* scale_factor, 0],[97* scale_factor, (365 - crop_h)* scale_factor],[160* scale_factor, (390 - crop_h)* scale_factor],[0, (462 - crop_h)* scale_factor], [0, 0]]),\n",
    "    #            np.array([[556* scale_factor, 0],[556* scale_factor, (365 - crop_h)* scale_factor],[504* scale_factor, (370 - crop_h)* scale_factor],[542* scale_factor, (444 - crop_h)* scale_factor], [640* scale_factor, (467 - crop_h)* scale_factor], [640* scale_factor, 0]])\n",
    "     #            ]\n",
    "rois = [np.array([[31, 452], [135, 391], [245, 398], [155, 484]]), np.array([[408, 400], [495, 416], [534, 523], [387, 503]]), \n",
    "        np.array([[425, 318], [499, 317], [528, 356], [418, 352]]), np.array([[184, 504], [346, 513], [384, 636], [37, 637]])]\n",
    "cap = cv2.VideoCapture(pth)\n",
    "ret,frame = cap.read()\n",
    "ans, wh, rois_ads = calculate_pre(rois)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() # destroy all opened windows\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01ee6d44-3592-4e01-8ffd-76350a097b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 (no detections), 84.8ms\n",
      "Speed: 1.6ms preprocess, 84.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 73.4ms\n",
      "Speed: 1.0ms preprocess, 73.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 76.0ms\n",
      "Speed: 1.6ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 74.6ms\n",
      "Speed: 1.4ms preprocess, 74.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 74.7ms\n",
      "Speed: 1.1ms preprocess, 74.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 73.6ms\n",
      "Speed: 2.0ms preprocess, 73.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 68.7ms\n",
      "Speed: 1.2ms preprocess, 68.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 75.0ms\n",
      "Speed: 2.7ms preprocess, 75.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 73.7ms\n",
      "Speed: 1.6ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 69.1ms\n",
      "Speed: 1.3ms preprocess, 69.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 71.4ms\n",
      "Speed: 1.2ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 large_bus, 84.2ms\n",
      "Speed: 1.9ms preprocess, 84.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 81.4ms\n",
      "Speed: 2.6ms preprocess, 81.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 81.2ms\n",
      "Speed: 1.3ms preprocess, 81.2ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 72.6ms\n",
      "Speed: 1.2ms preprocess, 72.6ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 67.4ms\n",
      "Speed: 1.3ms preprocess, 67.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 74.0ms\n",
      "Speed: 1.7ms preprocess, 74.0ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 72.8ms\n",
      "Speed: 1.5ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 77.3ms\n",
      "Speed: 1.6ms preprocess, 77.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 77.4ms\n",
      "Speed: 1.2ms preprocess, 77.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 74.7ms\n",
      "Speed: 1.1ms preprocess, 74.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 77.7ms\n",
      "Speed: 1.2ms preprocess, 77.7ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 79.5ms\n",
      "Speed: 1.2ms preprocess, 79.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 90.0ms\n",
      "Speed: 1.2ms preprocess, 90.0ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 77.5ms\n",
      "Speed: 1.4ms preprocess, 77.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 73.3ms\n",
      "Speed: 1.3ms preprocess, 73.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 73.4ms\n",
      "Speed: 1.3ms preprocess, 73.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 79.0ms\n",
      "Speed: 1.8ms preprocess, 79.0ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 86.4ms\n",
      "Speed: 1.8ms preprocess, 86.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 86.5ms\n",
      "Speed: 1.4ms preprocess, 86.5ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 84.2ms\n",
      "Speed: 2.0ms preprocess, 84.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m timings, p1 \u001b[38;5;241m=\u001b[39m \u001b[43mtime_profile_other_no_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovienocrop.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mtime_profile_other_no_crop\u001b[0;34m(path, ans, rois, imgsz)\u001b[0m\n\u001b[1;32m     19\u001b[0m old_frame \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     20\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# frame = predict_and_draw_on_frame(frame, ((640 - crop_h)* scale_factor, 640* scale_factor))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/engine/model.py:235\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/engine/predictor.py:194\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/engine/predictor.py:253\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 253\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/engine/predictor.py:133\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    131\u001b[0m     visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[1;32m    132\u001b[0m                                mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:339\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    336\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/modules/head.py:47\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m shape \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# BCHW\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[0;32m---> 47\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timings, p1 = time_profile_other_no_crop('movienocrop.mp4', ans, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff5557-982c-4900-b753-30fd58dc23f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timings_2, p2 = time_profile_other_crop('movienocrop.mp4', ans, rois_ads, wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db6f55-d73d-4ab0-8692-7f89e921287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timings, label=\"crop and whited out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363a0a2-a67d-475d-a874-7ba7228e2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timings_2, label=\"no processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f342b3-021d-4038-99dd-f57145183e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.ylim(0, 0.003)\n",
    "plt.plot(timings, label=\"segmented but not cropped\")\n",
    "plt.plot(timings_2, label=\"segmented and cropped\")\n",
    "plt.legend()\n",
    "print(np.average(timings))\n",
    "print(np.average(timings_2))\n",
    "df = pd.DataFrame()\n",
    "df['segmented but not cropped'] = timings\n",
    "df['segmented and cropped'] = timings_2\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479503f-6d19-4fbb-a87d-c1a8ab5b847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.plot(p1, label=\"processing segmented but not cropped\")\n",
    "plt.plot(p2, label=\"processing segmented and cropped\")\n",
    "plt.legend()\n",
    "print(np.average(p1))\n",
    "print(np.average(p2))\n",
    "print(np.average(p1) / np.average(p2))\n",
    "df = pd.DataFrame()\n",
    "df['processing segmented but not cropped'] = p1\n",
    "df['processing segmented and cropped'] = p2\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5abac-eda9-4ab7-8aed-47ce78bb1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('framenocrop.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705d8d6-d1a9-4835-9cc3-aa1d9c1a997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('frameoccrop.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dba85-4f03-4fa4-8f95-5545e2f2c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.ylim(0.04, 0.1)\n",
    "plt.plot(p1, label=\"processing segmented but not cropped\")\n",
    "plt.plot(p2, label=\"processing segmented and cropped\")\n",
    "plt.legend()\n",
    "print(np.average(p1))\n",
    "print(np.average(p2))\n",
    "df = pd.DataFrame()\n",
    "df['processing segmented but not cropped'] = p1\n",
    "df['processing segmented and cropped'] = p2\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3aee78-c1a2-4458-8ab6-b042f9893583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.ylim(0, 0.003)\n",
    "plt.plot(timings, label=\"segmented but not cropped\")\n",
    "plt.plot(timings_2, label=\"segmented and cropped\")\n",
    "plt.legend()\n",
    "print(np.average(timings))\n",
    "print(np.average(timings_2))\n",
    "df = pd.DataFrame()\n",
    "df['segmented but not cropped'] = timings\n",
    "df['segmented and cropped'] = timings_2\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9a944-ecb9-495b-b5e0-d56dd8a42c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('framenocrop.jpg')\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c644c-4d16-49d6-9c96-280f6d9d4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('frameoccrop.jpg')\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d6d83-6056-4572-b860-436ad4a52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ec5d7-854d-4ddc-8b86-bfa10d27c6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
